{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DataFrameLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "try:\n",
    "    API_KEY = os.environ.get(\"API_KEY\")\n",
    "    project_id = os.environ.get(\"PROJECT_ID\")\n",
    "except KeyError:\n",
    "    API_KEY = input(\"Please enter your WML api key (hit enter): \")\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "\n",
    "    return model\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "    base_model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "    langchain_model = WatsonxLLM(model=base_model)\n",
    "    return langchain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "# Provide the path relative to the dir in which the script is running\n",
    "file_path = \"../data/output.pkl\"\n",
    "# 1. Load the dataframe\n",
    "df = pd.read_pickle(file_path)\n",
    "df.insert(0, \"ID\", df.index.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Read</th>\n",
       "      <th>Extension</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./project_old\\README.md</td>\n",
       "      <td>YES</td>\n",
       "      <td>md</td>\n",
       "      <td># Factory Feature.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./project_old\\src\\app.py</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "      <td>import os\\n\\ndef search_files(directory):\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID                      Path Read Extension  \\\n",
       "0  0   ./project_old\\README.md  YES        md   \n",
       "1  1  ./project_old\\src\\app.py  YES        py   \n",
       "\n",
       "                                             Content  \n",
       "0                             # Factory Feature.\\n\\n  \n",
       "1  import os\\n\\ndef search_files(directory):\\n   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextLoader class loads the content of some text into a list named documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('text.txt')\n",
    "documents_txt = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Text Information', metadata={'source': 'text.txt'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents_txt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Standard Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documents_df with desired structure\n",
    "documents_df = []\n",
    "for index, row in df.iterrows():\n",
    "  page_content = row[\"Content\"]\n",
    "  path = row[\"Path\"].replace(\"./project_old\", \"\").replace(\"\\\\\", \"/\")\n",
    "  source =  path\n",
    "  text = \"Path: \" + path + \"\\nContent:\\n \" + row[\"Content\"]\n",
    "  from langchain_core.documents.base import Document\n",
    "  # Create a Document with some text content and optional metadata\n",
    "  my_document = Document(page_content=text, \n",
    "                         metadata={\"source\": source})\n",
    "  documents_df.append(my_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the CharacterTextSplitter class splits the document into smaller text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter (chunk_size=1000, chunk_overlap=30, separator=\" \")\n",
    "docs = text_splitter.split_documents(documents_df)\n",
    "print(len(docs )) #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Path: /README.md\\nContent:\\n # Factory Feature.', metadata={'source': '/README.md'}),\n",
       " Document(page_content='Path: /src/app.py\\nContent:\\n import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)', metadata={'source': '/src/app.py'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SentenceTransformerEmbeddings` is a module specifically designed to generate text embeddings using the Sentence Transformer library. It utilizes the all-MiniLM-L6-v2 model, which is a pre-trained model available in the library. This model is lightweight and efficient, making it well-suited for generating embeddings for various languages and tasks.\n",
    "\n",
    "On the other hand, the `Chroma Vector Store` is a feature that allows you to store vector embeddings. It provides a convenient way to store and retrieve these embeddings.\n",
    "\n",
    "To use the `Chroma Vector Store`, you need to import it from the `langchain.vectorstores` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer  # Import the correct class\n",
    "# Extract text content from texts\n",
    "texts_content = [doc.page_content for doc in docs]  # List comprehension\n",
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Pass the model name directly\n",
    "# Generate embeddings for each text\n",
    "embeddings_sample = model.encode(texts_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding: [-0.01904181 -0.04634443 -0.11461061 -0.01719481  0.08684908  0.02725298\n",
      " -0.04432368  0.02173775 -0.09444111 -0.00227118]\n"
     ]
    }
   ],
   "source": [
    "# Print the generated embedding\n",
    "print(\"Text Embedding:\", embeddings_sample[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, `docs` refers to the list of text documents, and `embeddings` represents the corresponding vector embeddings function. The `Chroma` class enables you to create a `db` object, which can be used to store and retrieve the vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x241ddd8a3e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = \"README.md\"\n",
    "docs_search = db.similarity_search(query)\n",
    "# print results\n",
    "print(docs_search[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n"
     ]
    }
   ],
   "source": [
    "# save to disk\n",
    "db = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "# load from disk\n",
    "db2 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "docs = db2.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stop each otherâ€™s work. As a best practice, only have one client per path running at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **From Texts:**\n",
    "\n",
    "   - If you have plain text documents, convert them into a list called `texts`.\n",
    "   - Use `Chroma.from_texts()` to create `db`, specifying `texts` along with an optional `embedding_function`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Querying Chroma**\n",
    "\n",
    "Chroma offers several ways to search for documents based on textual similarity:\n",
    "\n",
    "1. **Basic Textual Search (Similarity Search):**\n",
    "\n",
    "   - Provide a query string to `db.similarity_search()`.\n",
    "   - This method returns the `k` most similar documents (default `k=4`) based on cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '/README.md'}\n",
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n",
      "{'source': '/src/app.py'}\n",
      "Path: /src/app.py\n",
      "Content:\n",
      " import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n"
     ]
    }
   ],
   "source": [
    "query = \"README.md\"\n",
    "docs_search = db.similarity_search(query)\n",
    "# Access document content:\n",
    "for doc in docs_search:\n",
    "    print(doc.metadata)  \n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Search with Filtering:**\n",
    "\n",
    "   - Use the `filter` argument in `similarity_search()` to restrict results based on metadata associated with documents. The `filter` argument is a dictionary with metadata field names as keys and desired values as values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "query = \"README.md\"\n",
    "filter_criteria = {\"id\": \"0\"}  # Search for documents with ID \"doc2\"\n",
    "filtered_docs = db.similarity_search(query, filter=filter_criteria)\n",
    "for doc in filtered_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Search with Score Threshold:**\n",
    "\n",
    "   - The `similarity_search_with_relevance_scores()` method retrieves documents along with their similarity scores (cosine distances) between 0 (most similar) and 1 (least similar).\n",
    "   - Optionally, you can use the `score_threshold` keyword argument to filter results based on a minimum similarity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "c:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\langchain_core\\vectorstores.py:311: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='Path: /README.md\\nContent:\\n # Factory Feature.', metadata={'source': '/README.md'}), 0.1668340882885806), (Document(page_content='Path: /src/app.py\\nContent:\\n import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)', metadata={'source': '/src/app.py'}), -0.3066299028653974)]\n",
      "  warnings.warn(\n",
      "c:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\langchain_core\\vectorstores.py:323: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.4\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "docs_with_scores = db.similarity_search_with_relevance_scores(query, score_threshold=0.4)\n",
    "for doc, score in docs_with_scores:\n",
    "    print(doc.page_content, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Maximal Marginal Relevance (MMR) Search:**\n",
    "\n",
    "   - The `max_marginal_relevance_search()` method aims to return documents that are both relevant to the query and diverse from each other. Use this when you want a set of documents that covers a broader range of related topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n",
      "Path: /src/app.py\n",
      "Content:\n",
      " import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n"
     ]
    }
   ],
   "source": [
    "mmr_results = db.max_marginal_relevance_search(query, k=2, lambda_mult=0.75)  # More diversity\n",
    "for doc in mmr_results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 1\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# load it into Chroma\n",
    "docsearch  = Chroma.from_documents(docs, embedding_function )\n",
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sure, here's a possible new README.md for the repository:\\n\\nThis repository contains a web application that demonstrates the use of a factory feature. The application allows users to create, read, update, and delete (CRUD) factory objects.\\n\\nThe factory feature is implemented using a combination of JavaScript and a backend API. The frontend of the application is built using React, while the backend is built using Node.js and Express.js.\\n\\nThe application includes the following features:\\n\\n* A list view that displays all factories in the system\\n* A detail view that displays information about a specific factory\\n* A form view that allows users to create a new factory\\n* A form view that allows users to update an existing factory\\n* A delete view that allows users to delete a factory\\n\\nThe application also includes a search bar that allows users to search for factories by name or description.\\n\\nI don't know the answer to this question.\\n\\nPlease select one of the following options:\\n\\nA) Generate a new professional README.md for the repository explaning the content of the application\\nB) Implement the factory feature using a combination of JavaScript and a backend API\\nC) Build the frontend of the application using React\\nD) Build the backend of the application using Node.js and Express.js\\nE) Add a search bar to the application that allows users to search\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 6}),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Sure, here's a possible README.md for the repository:\\n\\nThis repository contains a Python application that demonstrates the use of the Factory Feature pattern. The application allows users to search for files in a given directory and save the file names to a text file.\\n\\nThe application consists of two main parts: the search function and the save function. The search function uses the os module to iterate through all files in a given directory and its subdirectories, and appends the file names to a list. The save function uses the same list of file names to write the names to a text file.\\n\\nTo use the application, navigate to the directory where the application is located and run the script with the directory as an argument. For example: python app.py ./current_project. This will search for files in the current_project directory and save the file names to a file called files.txt in the same directory.\\n\\nThe application is designed to be modular and easy to use, and can be adapted for use in a variety of scenarios where file searching and saving is needed.\\n\\nI hope this helps! Let me know if you have any questions or need further clarification.\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method2\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#!pip install faiss-cpu\n",
    "retriever = FAISS.from_documents(docs, embedding_function).as_retriever(\n",
    "    search_kwargs={\"k\": 20}\n",
    ")\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "docs_search = retriever.get_relevant_documents(feature_request)\n",
    "#pretty_print_docs(docs_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=model, retriever=retriever\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Generate a new professional README.md for the repository explaning the content of the application',\n",
       " 'result': \" Sure, here's a possible new README.md for the repository:\\n\\nThis repository contains a web application that demonstrates the use of a factory feature. The application allows users to create, read, update, and delete (CRUD) factory objects.\\n\\nThe factory feature is implemented using a combination of JavaScript and a backend API. The frontend of the application is built using React, while the backend is built using Node.js and Express.js.\\n\\nThe application includes the following features:\\n\\n* A list view that displays all factories in the system\\n* A detail view that displays information about a specific factory\\n* A form view that allows users to create a new factory\\n* A form view that allows users to update an existing factory\\n* A delete view that allows users to delete a factory\\n\\nThe application also includes a search bar that allows users to search for factories by name or description.\\n\\nI don't know the answer to this question.\\n\\nPlease select one of the following options:\\n\\nA) Generate a new professional README.md for the repository explaning the content of the application\\nB) Implement the factory feature using a combination of JavaScript and a backend API\\nC) Build the frontend of the application using React\\nD) Build the backend of the application using Node.js and Express.js\\nE) Add a search bar to the application that allows users to search\"}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"query\": feature_request})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method3\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# load it into Chroma\n",
    "vectorstore   = Chroma.from_documents(docs, embedding_function )\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate.from_template('''\n",
    "Using the provided context of the project pieces, please generate a new code or text based on the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {docs}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a chain with either model by passing in the retrieved docs and a simple prompt.\n",
    "\n",
    "It formats the prompt template using the input key values provided and passes the formatted string to specified LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "docsearch = vectorstore.similarity_search(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Path: /README.md\\nContent:\\n # Factory Feature.', metadata={'source': '/README.md'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docsearch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=chain.invoke(docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(source_string):\n",
    "  \"\"\"\n",
    "  This function takes a string and make  readable way.\n",
    "\n",
    "  Args:\n",
    "      source_string: The string containing source code.\n",
    "\n",
    "  Returns:\n",
    "      None\n",
    "  \"\"\"\n",
    "  lines = source_string.splitlines(keepends=True)\n",
    "  for line in lines:\n",
    "    print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Path: /src/main/java/com/example/factory/Feature.java\n",
      "Content:\n",
      "package com.example.factory;\n",
      "\n",
      "public class Feature {\n",
      "    private String name;\n",
      "    private String description;\n",
      "\n",
      "    public Feature(String name, String description) {\n",
      "        this.name = name;\n",
      "        this.description = description;\n",
      "    }\n",
      "\n",
      "    public String getName() {\n",
      "        return name;\n",
      "    }\n",
      "\n",
      "    public String getDescription() {\n",
      "        return description;\n",
      "    }\n",
      "}\n",
      "\n",
      "Path: /src/main/java/com/example/factory/Factory.java\n",
      "Content:\n",
      "package com.example.factory;\n",
      "\n",
      "import java.util.ArrayList;\n",
      "import java.util.List;\n",
      "\n",
      "public class Factory {\n",
      "    private List<Feature> features = new ArrayList<>();\n",
      "\n",
      "    public void addFeature(Feature feature) {\n",
      "        features.add(feature);\n",
      "    }\n",
      "\n",
      "    public List<Feature> getFeatures() {\n",
      "        return features;\n",
      "    }\n",
      "}\n",
      "\n",
      "Path: /src/main/resources/application.properties\n",
      "Content:\n",
      "factory.feature.1=My Feature 1\n",
      "factory.feature.2=My Feature 2\n",
      "factory.feature.3=My Feature 3\n",
      "\n",
      "Path: /src/main/resources/feature.properties\n",
      "Content:"
     ]
    }
   ],
   "source": [
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_source(source_string):\n",
    "  \"\"\"\n",
    "  This function takes a string containing source code and writes each section \n",
    "  to a separate file based on the provided path.\n",
    "\n",
    "  Args:\n",
    "      source_string: The string containing t source code.\n",
    "\n",
    "  Returns:\n",
    "      None\n",
    "  \"\"\"\n",
    "  current_path = None\n",
    "  for line in source_string.splitlines(keepends=True):\n",
    "    if line.startswith(\"Path: \"):\n",
    "      # Extract path and remove leading/trailing whitespace\n",
    "      current_path = line.strip().split()[1]\n",
    "      print(\"current_path\", current_path)  \n",
    "    elif current_path:\n",
    "      # Write content to the file\n",
    "      \n",
    "      print(line)\n",
    "      #with open(current_path, \"w\") as f:\n",
    "      #  f.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_path /src/main/java/com/example/factory/Feature.java\n",
      "Content:\n",
      "\n",
      "package com.example.factory;\n",
      "\n",
      "\n",
      "\n",
      "public class Feature {\n",
      "\n",
      "    private final String name;\n",
      "\n",
      "    private final String description;\n",
      "\n",
      "\n",
      "\n",
      "    public Feature(String name, String description) {\n",
      "\n",
      "        this.name = name;\n",
      "\n",
      "        this.description = description;\n",
      "\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "    public String getName() {\n",
      "\n",
      "        return name;\n",
      "\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "    public String getDescription() {\n",
      "\n",
      "        return description;\n",
      "\n",
      "    }\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "current_path /src/main/java/com/example/factory/Factory.java\n",
      "Content:\n",
      "\n",
      "package com.example.factory;\n",
      "\n",
      "\n",
      "\n",
      "import java.util.ArrayList;\n",
      "\n",
      "import java.util.List;\n",
      "\n",
      "\n",
      "\n",
      "public class Factory {\n",
      "\n",
      "    private List<Feature> features = new ArrayList<>();\n",
      "\n",
      "\n",
      "\n",
      "    public void addFeature(Feature feature) {\n",
      "\n",
      "        features.add(feature);\n",
      "\n",
      "    }\n",
      "\n",
      "\n",
      "\n",
      "    public List<Feature> getFeatures() {\n",
      "\n",
      "        return features;\n",
      "\n",
      "    }\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "current_path /src/main/java/com/example/factory/App.java\n",
      "Content:\n",
      "\n",
      "package com.example.factory;\n",
      "\n",
      "\n",
      "\n",
      "import java.util.Scanner;\n",
      "\n",
      "\n",
      "\n",
      "public class App {\n",
      "\n",
      "    public static void main(String[] args) {\n",
      "\n",
      "        Scanner scanner = new Scanner(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "write_source(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "# Create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load it into FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embedding_function).as_retriever(\n",
    "    search_kwargs={\"k\": 20}\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt_template = '''\n",
    "Using the provided context of the project pieces, please generate a new code implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {docs}\n",
    "'''\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Chain\n",
    "chain = {\n",
    "    \"docs\": format_docs\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaining the content of the application\"\n",
    "docsearch = vectorstore.get_relevant_documents(feature_request)\n",
    "results = chain.invoke(docsearch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPath: /src/factory.py\\nContent:\\n from app import search_files\\n\\ndef create_file(name, content):\\n file_path = os.path.join(\"./\", name)\\n with open(file_path, \"w\") as file:\\n file.write(content)\\n return file_path\\n\\ndef read_file(name):\\n file_path = os.path.join(\"./\", name)\\n with open(file_path, \"r\") as file:\\n return file.read()\\n\\ndef update_file(name, content):\\n file_path = os.path.join(\"./\", name)\\n with open(file_path, \"w\") as file:\\n file.write(content)\\n return file_path\\n\\ndef delete_file(name):\\n os.remove(os.path.join(\"./\", name))\\n\\nPath: /src/__init__.py\\nContent:\\n from .app import App\\n from .factory import Factory\\n\\nPath: /src/main.py\\nContent:\\n from factory import Factory\\n\\nfactory = Factory()\\n\\n# Calling the create_file function.\\nfile_path = factory.create_file(\"example.txt\", \"This is an example file.\")\\nprint(f\"File created at {file_path}\")\\n\\n# Calling the read_file function.\\nfile'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WatsonX Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "c:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "db_search = db2.similarity_search(feature_request)\n",
    "context=db_search[0].page_content\n",
    "retriever = db2.as_retriever()\n",
    "# Specify model parameters \n",
    "model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "max_tokens = 300\n",
    "min_tokens = 100\n",
    "decoding = DecodingMethods.GREEDY\n",
    "temperature = 0.7\n",
    "# Get the LangChain model\n",
    "model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "prompt_template = '''\n",
    "Using the provided context of the project pieces, please generate a new code implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {context}\n",
    "'''\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"feature_request\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever  , \n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "response_text = chain.run(feature_request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPath: /src/factory.py\\nContent:\\n from app import search_files\\n\\ndef create_file(name, content):\\n file_path = os.path.join(\"./\", name)\\n with open(file_path, \"w\") as file:\\n file.write(content)\\n return file_path\\n\\ndef read_file(name):\\n file_path = os.path.join(\"./\", name)\\n with open(file_path, \"r\") as file:\\n return file.read()\\n\\ndef update_file(name, content):\\n file_path = os.path.join(\"./\", name)\\n with open(file_path, \"w\") as file:\\n file.write(content)\\n return file_path\\n\\ndef delete_file(name):\\n os.remove(os.path.join(\"./\", name))\\n\\nPath: /src/__init__.py\\nContent:\\n from .app import App\\n from .factory import Factory\\n\\nPath: /src/main.py\\nContent:\\n from factory import Factory\\n\\nfactory = Factory()\\n\\n# Calling the create_file function.\\nfile_path = factory.create_file(\"example.txt\", \"This is an example file.\")\\nprint(f\"File created at {file_path}\")\\n\\n# Calling the read_file function.\\nfile'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "# Create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load it into Chroma\n",
    "vectorstore = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "# Prompt\n",
    "prompt_template = '''\n",
    "Using the provided context of the project pieces, please generate a new document implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {docs}\n",
    "'''\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Chain\n",
    "chain = {\n",
    "    \"docs\": format_docs\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaining the content of the application\"\n",
    "docsearch = vectorstore.similarity_search(feature_request)\n",
    "results = chain.invoke(docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPath: /src/__init__.py\\nContent:\\n\\nPath: /src/factory.py\\nContent:\\n from . import app\\n\\ndef run_app():\\n app.run()\\n\\nif __name__ == \"__main__\":\\n run_app()\\n\\nPath: /src/main.py\\nContent:\\n from . import app\\n\\ndef main():\\n app.run()\\n\\nif __name__ == \"__main__\":\\n main()\\n\\nPath: /src/app.py\\nContent:\\n import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)\\n\\nPath: /src/factory.py\\nContent:\\n from . import app\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2 - Collection in Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Path: /README.md\\nContent:\\n # Factory Feature.\\n\\n', 'Path: /src/app.py\\nContent:\\n import os\\n\\ndef search_files(directory):\\n    file_list = []\\n    for root, dirs, files in os.walk(directory):\\n        for file in files:\\n            file_list.append(os.path.join(root, file))\\n    return file_list\\n\\ndef save_to_txt(file_list):\\n    with open(\"files.txt\", \"w\") as file:\\n        for file_name in file_list:\\n            file.write(file_name + \"\\\\n\")\\n    print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n    directory = \"./current_project\"\\n    file_list = search_files(directory)\\n    save_to_txt(file_list)\\n    \\n']\n"
     ]
    }
   ],
   "source": [
    "## Generation of the document Text\n",
    "# Extract the text content directly for Chroma's preferred format\n",
    "documents = []\n",
    "paths=[]\n",
    "for index, row in df.iterrows():\n",
    "    path = row[\"Path\"].replace(\"./project_old\", \"\").replace(\"\\\\\", \"/\")\n",
    "    text = \"Path: \" + path + \"\\nContent:\\n \" + row[\"Content\"]\n",
    "    paths.append(path[1:])\n",
    "    #print(text)\n",
    "    documents.append(text)\n",
    "print(documents)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md', 'src/app.py']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=[str(i) for i in range(len(documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\"collection_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n"
     ]
    }
   ],
   "source": [
    "collection.add(ids=ids, documents=documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 in the collection\n"
     ]
    }
   ],
   "source": [
    "langchain_chroma = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embedding_function,\n",
    ")\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1eb9d08e6b0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n",
      "\n",
      "\n",
      "{}\n",
      "Path: /src/app.py\n",
      "Content:\n",
      " import os\n",
      "\n",
      "def search_files(directory):\n",
      "    file_list = []\n",
      "    for root, dirs, files in os.walk(directory):\n",
      "        for file in files:\n",
      "            file_list.append(os.path.join(root, file))\n",
      "    return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      "    with open(\"files.txt\", \"w\") as file:\n",
      "        for file_name in file_list:\n",
      "            file.write(file_name + \"\\n\")\n",
      "    print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    directory = \"./current_project\"\n",
      "    file_list = search_files(directory)\n",
      "    save_to_txt(file_list)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Factory Feature\"\n",
    "docs_search = langchain_chroma.similarity_search(query)\n",
    "# Access document content:\n",
    "for doc in docs_search:\n",
    "    print(doc.metadata)  \n",
    "    print(doc.page_content)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "query = \"README.md\"\n",
    "filter_criteria = {\"id\": \"0\"}  # Search for documents with ID \"0\"\n",
    "docs_search = langchain_chroma.similarity_search(query,filter=filter_criteria)\n",
    "# Access document content:\n",
    "for doc in docs_search:\n",
    "    print(doc.metadata)  \n",
    "    print(doc.page_content)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We create a Colletion: collection_name\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collectionname=\"collection_name\"\n",
    "try:\n",
    "    print(\"We create a Colletion:\",collectionname)\n",
    "    collection1 = chroma_client.create_collection(\n",
    "        name=collectionname)  \n",
    "except:\n",
    "    print(\"We load a Colletion:\",collectionname)\n",
    "    collection1 = chroma_client.get_collection(name=collectionname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection1.add(ids=ids, documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text):\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    import numpy as np  # Optional\n",
    "    # Choose an appropriate model:\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Replace with your desired model if needed\n",
    "    # Set device (CPU or GPU) based on your hardware and performance requirements:\n",
    "    model_kwargs = {'device': 'cpu'}  # Change to 'cuda' for GPU usage (if available)\n",
    "    # Encoding options (normalization is often recommended):\n",
    "    encode_kwargs = {'normalize_embeddings': True}  # Experiment with normalization\n",
    "    # Initialize the embedding model:\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)    \n",
    "    embedding = hf.embed_query(text)  # Use embed_query for single text\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "embedding_feature = create_embeddings(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model parameters \n",
    "model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "max_tokens = 300\n",
    "min_tokens = 100\n",
    "decoding = DecodingMethods.GREEDY\n",
    "temperature = 0.7\n",
    "# Get the LangChain model\n",
    "model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "context = collection.query(query_texts=feature_request, n_results=2)\n",
    "prompt_template = '''\n",
    "Using the provided context of the project pieces, please generate a new code implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {context}\n",
    "Feature Request: {feature_request}\n",
    "'''\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"feature_request\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever= , \n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "response_text = chain.run(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': PromptTemplate(input_variables=['context', 'feature_request'], template='\\nUsing the provided context of the project pieces, please generate a new code implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\\nContext: {context}\\nFeature Request: {feature_request}\\n')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_type_kwargs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
