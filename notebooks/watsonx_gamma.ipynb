{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1  Creation of the Dataframe of the project\n",
    "The project example is at the ..src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add the utils directory to the system path\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "# Now you can import the function\n",
    "from utils.extractor import display_and_store_directory_content \n",
    "# Call the function with the desired path\n",
    "display_and_store_directory_content('../src') \n",
    "clear_output()  # Clears the output after the current line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>readable</th>\n",
       "      <th>extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../src/analysis</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../src/generation</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../src/models</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../src/utils</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../src/vector_database</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     path content readable extension\n",
       "0         ../src/analysis              N/A       N/A\n",
       "1       ../src/generation              N/A       N/A\n",
       "2           ../src/models              N/A       N/A\n",
       "3            ../src/utils              N/A       N/A\n",
       "4  ../src/vector_database              N/A       N/A"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Replace 'your_file.pkl' with the actual path to your pickle file\n",
    "file_path = './extraction/src.pkl'\n",
    "# Load the DataFrame from the pickle file\n",
    "df = pd.read_pickle(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where 'readable' is not 'N/A'\n",
    "df_filtered = df[df['readable'] != 'N/A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['path', 'content', 'readable', 'extension'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the filtered DataFrame\n",
    "df_filtered.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>readable</th>\n",
       "      <th>extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>../src/__init__.py</td>\n",
       "      <td></td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>../src/analysis/dependency_resolver.py</td>\n",
       "      <td>import os\\n\\ndef resolve_dependencies(project_...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>../src/analysis/feature_mapper.py</td>\n",
       "      <td>def map_features_to_components(project_data, f...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>../src/analysis/project_parser.py</td>\n",
       "      <td>import os\\n\\ndef parse_project(project_path):\\...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>../src/analysis/__init__.py</td>\n",
       "      <td></td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      path  \\\n",
       "6                       ../src/__init__.py   \n",
       "8   ../src/analysis/dependency_resolver.py   \n",
       "9        ../src/analysis/feature_mapper.py   \n",
       "10       ../src/analysis/project_parser.py   \n",
       "11             ../src/analysis/__init__.py   \n",
       "\n",
       "                                              content readable extension  \n",
       "6                                                          YES        py  \n",
       "8   import os\\n\\ndef resolve_dependencies(project_...      YES        py  \n",
       "9   def map_features_to_components(project_data, f...      YES        py  \n",
       "10  import os\\n\\ndef parse_project(project_path):\\...      YES        py  \n",
       "11                                                         YES        py  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM Watsonx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Man's best friend is his dog. Dogs are man's best friend because they are always there for you, they never judge you, and they always want to play. Dogs are also very smart, and they can learn tricks and commands. Dogs are also very good at detecting danger, and they can help keep you safe. Dogs are man's best friend because they are always there for you. \n"
     ]
    }
   ],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch credentials or prompt the user if missing\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if not WATSONX_APIKEY:\n",
    "    WATSONX_APIKEY = input(\"WML API key not found in .env. Please enter your WML API key: \").strip()\n",
    "    print(\"Reminder: Save your WML API key to the .env file for future use.\")\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = input(\"Project ID not found in .env. Please enter your project ID: \").strip()\n",
    "    print(\"Reminder: Save your Project ID to the .env file for future use.\")\n",
    "\n",
    "# Watsonx credentials\n",
    "credentials = {\n",
    "    \"url\": \"https://eu-gb.ml.cloud.ibm.com\",  # Update the URL as required\n",
    "    \"apikey\": WATSONX_APIKEY,\n",
    "    \"project_id\": PROJECT_ID,\n",
    "}\n",
    "\n",
    "# Example parameters for WatsonxLLM\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "# Initialize WatsonxLLM\n",
    "try:\n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        url=credentials[\"url\"],\n",
    "        project_id=credentials[\"project_id\"],\n",
    "        params=parameters,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error initializing WatsonxLLM: {e}\")\n",
    "\n",
    "# Function to invoke the WatsonxLLM model\n",
    "def invoke_model(prompt):\n",
    "    \"\"\"\n",
    "    Invokes the WatsonxLLM model with the given prompt.\n",
    "\n",
    "    :param prompt: The input prompt for the model.\n",
    "    :return: The response from the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = watsonx_llm.invoke(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during model invocation: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample prompt for the model\n",
    "    prompt = \"Who is man's best friend?\"\n",
    "    try:\n",
    "        # Get the model's response\n",
    "        result = invoke_model(prompt)\n",
    "        print(f\"Response: {result}\")\n",
    "    except RuntimeError as e:\n",
    "        # Print any errors encountered\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 is the enhacement of the metadata of each element of the project.\n",
    "We create a column with a small description about the content using llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions generated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch credentials or prompt the user if missing\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if not WATSONX_APIKEY:\n",
    "    WATSONX_APIKEY = input(\"WML API key not found in .env. Please enter your WML API key: \").strip()\n",
    "    print(\"Reminder: Save your WML API key to the .env file for future use.\")\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = input(\"Project ID not found in .env. Please enter your project ID: \").strip()\n",
    "    print(\"Reminder: Save your Project ID to the .env file for future use.\")\n",
    "\n",
    "# Watsonx credentials\n",
    "credentials = {\n",
    "    \"url\": \"https://eu-gb.ml.cloud.ibm.com\",  # Update the URL as required\n",
    "    \"apikey\": WATSONX_APIKEY,\n",
    "    \"project_id\": PROJECT_ID,\n",
    "}\n",
    "\n",
    "# Initialize WatsonxLLM\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "try:\n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        url=credentials[\"url\"],\n",
    "        project_id=credentials[\"project_id\"],\n",
    "        params=parameters,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error initializing WatsonxLLM: {e}\")\n",
    "\n",
    "# Function to invoke the WatsonxLLM model\n",
    "def invoke_model(prompt):\n",
    "    try:\n",
    "        response = watsonx_llm.invoke(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during model invocation: {e}\")\n",
    "\n",
    "# Function to generate a description for each text element\n",
    "def generate_description(df):\n",
    "    \"\"\"\n",
    "    Generates descriptions for each row in the DataFrame by parsing the 'text' column\n",
    "    and invoking WatsonxLLM for enhanced metadata creation.\n",
    "\n",
    "    :param df: The DataFrame containing the 'text' column.\n",
    "    :return: A new DataFrame with an added 'description' column.\n",
    "    \"\"\"\n",
    "    descriptions = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        prompt = f\"Provide a detailed summary description for the following content: {text}\"\n",
    "        try:\n",
    "            description = invoke_model(prompt)\n",
    "            descriptions.append(description)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error generating description for row: {e}\")\n",
    "            descriptions.append(\"Error generating description\")\n",
    "\n",
    "    # Add the descriptions to the DataFrame\n",
    "    df['description'] = descriptions\n",
    "    return df\n",
    "\n",
    "# Main logic\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the DataFrame\n",
    "    file_path = './extraction/src.pkl'\n",
    "    df = pd.read_pickle(file_path)\n",
    "\n",
    "    # Create the 'text' column\n",
    "    def create_text_column(row):\n",
    "        text = f\"path: {row['path']} content: {row['content']} readable: {row['readable']} extension: {row['extension']}\"\n",
    "        return text\n",
    "\n",
    "    df['text'] = df.apply(create_text_column, axis=1)\n",
    "\n",
    "    # Generate descriptions and save results\n",
    "    try:\n",
    "        enhanced_df = generate_description(df)\n",
    "        enhanced_df.to_pickle(\"enhanced_dataframe.pkl\")\n",
    "        print(\"Descriptions generated and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>readable</th>\n",
       "      <th>extension</th>\n",
       "      <th>text</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../src/analysis</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>path: ../src/analysis content:  readable: N/A ...</td>\n",
       "      <td>The filepath ../src/analysis does not exist or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../src/generation</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>path: ../src/generation content:  readable: N/...</td>\n",
       "      <td>The path ../src/generation does not contain an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../src/models</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>path: ../src/models content:  readable: N/A ex...</td>\n",
       "      <td>models: N/A extension: N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../src/utils</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>path: ../src/utils content:  readable: N/A ext...</td>\n",
       "      <td>license: N/Apath: ../../src/utils</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../src/vector_database</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>path: ../src/vector_database content:  readabl...</td>\n",
       "      <td>The path ../src/vector_database is readable N/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     path content readable extension  \\\n",
       "0         ../src/analysis              N/A       N/A   \n",
       "1       ../src/generation              N/A       N/A   \n",
       "2           ../src/models              N/A       N/A   \n",
       "3            ../src/utils              N/A       N/A   \n",
       "4  ../src/vector_database              N/A       N/A   \n",
       "\n",
       "                                                text  \\\n",
       "0  path: ../src/analysis content:  readable: N/A ...   \n",
       "1  path: ../src/generation content:  readable: N/...   \n",
       "2  path: ../src/models content:  readable: N/A ex...   \n",
       "3  path: ../src/utils content:  readable: N/A ext...   \n",
       "4  path: ../src/vector_database content:  readabl...   \n",
       "\n",
       "                                         description  \n",
       "0  The filepath ../src/analysis does not exist or...  \n",
       "1  The path ../src/generation does not contain an...  \n",
       "2                         models: N/A extension: N/A  \n",
       "3                  license: N/Apath: ../../src/utils  \n",
       "4  The path ../src/vector_database is readable N/...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Creation of the vector database from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Blog/Building-LLM-from-Scratch-in-Python/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 48 documents to the collection.\n",
      "Collection 'my_vector_collection' now contains 48 documents.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Create the 'text' column\n",
    "def create_text_column(row):\n",
    "    text = f\"path: {row['path']} content: {row['content']} readable: {row['readable']} extension: {row['extension']}\"\n",
    "    return text\n",
    "\n",
    "# Define file path and collection name\n",
    "file_path = \"./extraction/src.pkl\"\n",
    "collection_name = \"my_vector_collection\"\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_pickle(file_path)\n",
    "df['text'] = df.apply(create_text_column, axis=1)\n",
    "df.insert(0, \"ID\", df.index.astype(str))  # Add unique ID for each row\n",
    "\n",
    "# Correct client initialization for ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Use PersistentClient for on-disk storage\n",
    "\n",
    "# Ensure the collection is deleted if it already exists\n",
    "if collection_name in [col.name for col in chroma_client.list_collections()]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "# Create a new collection\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "def create_embeddings_and_store(df, page_content_column, collection):\n",
    "    \"\"\"\n",
    "    This function generates embeddings from a dataframe and stores them in a Chroma collection.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The dataframe containing text data.\n",
    "        page_content_column (str): The name of the column containing the text content.\n",
    "        collection (chromadb.Collection): The Chroma collection to store the embeddings.\n",
    "    \"\"\"\n",
    "    # Configure embedding model\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_kwargs = {'device': device}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "    # Extract text content and generate embeddings\n",
    "    documents = df[page_content_column].tolist()\n",
    "    ids = df[\"ID\"].tolist()\n",
    "\n",
    "    # Generate embeddings for all documents\n",
    "    embeddings = hf.embed_documents(documents)  # Batch embedding generation\n",
    "\n",
    "    # Add documents, embeddings, and IDs to the collection\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"index\": i} for i in range(len(documents))]  # Example metadata\n",
    "    )\n",
    "\n",
    "    print(f\"Successfully added {len(documents)} documents to the collection.\")\n",
    "\n",
    "# Create embeddings and store them in Chroma\n",
    "create_embeddings_and_store(df, \"text\", collection)\n",
    "\n",
    "# Verify stored data\n",
    "print(f\"Collection '{collection_name}' now contains {collection.count()} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first 5 documents:\n",
      "Document ID: 0\n",
      "Content: path: ../src/analysis content:  readable: N/A extension: N/A\n",
      "Metadata: {'index': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "Document ID: 1\n",
      "Content: path: ../src/generation content:  readable: N/A extension: N/A\n",
      "Metadata: {'index': 1}\n",
      "--------------------------------------------------------------------------------\n",
      "Document ID: 2\n",
      "Content: path: ../src/models content:  readable: N/A extension: N/A\n",
      "Metadata: {'index': 2}\n",
      "--------------------------------------------------------------------------------\n",
      "Document ID: 3\n",
      "Content: path: ../src/utils content:  readable: N/A extension: N/A\n",
      "Metadata: {'index': 3}\n",
      "--------------------------------------------------------------------------------\n",
      "Document ID: 4\n",
      "Content: path: ../src/vector_database content:  readable: N/A extension: N/A\n",
      "Metadata: {'index': 4}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def display_first_five_documents(collection):\n",
    "    \"\"\"\n",
    "    Reads the first 5 documents from the given ChromaDB collection and displays them.\n",
    "\n",
    "    Args:\n",
    "        collection (chromadb.Collection): The ChromaDB collection to read from.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Retrieve all documents from the collection\n",
    "    results = collection.get()\n",
    "    \n",
    "    # Extract documents and IDs\n",
    "    documents = results.get(\"documents\", [])\n",
    "    ids = results.get(\"ids\", [])\n",
    "    metadatas = results.get(\"metadatas\", [])\n",
    "\n",
    "    # Display the first 5 documents\n",
    "    print(f\"Displaying the first {min(len(documents), 5)} documents:\")\n",
    "    for i in range(min(len(documents), 5)):\n",
    "        print(f\"Document ID: {ids[i]}\")\n",
    "        print(f\"Content: {documents[i]}\")\n",
    "        if metadatas:\n",
    "            print(f\"Metadata: {metadatas[i]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Usage Example\n",
    "display_first_five_documents(collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Inference of the LLM by using the vector Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- Generated response -----------------------------------\n",
      "{'question': 'How to integrate the specified features into the project components?', 'result': ' You can use the `integrate_features` function from the `feature_integration.py` module, passing in the list of project components and the feature instructions as arguments. For example: `integrate_features(project_components=[\"component1.py\", \"component2.py\"], feature_instructions=\"import feature_module\\\\nfeature_module.do_something()\")`. This will append the feature instructions to the end of each project component file. \\n\\nNote: The `integrate_features` function does not return any value, it modifies the project components in-place. \\n\\nAlso, you can use the `map_features_to_components` function from the `feature_mapper.py` module to map feature requests to specific project components before integrating the features. \\n\\nFor example: `component_mapping = map_features_to_components(project_data=[{\"path\": \"component1.py\", \"content\": \"This is component 1\"}, {\"path\": \"component2.py\", \"content\": \"This is component 2\"}], feature_request=\"feature_request\")`. \\n\\nThen you can pass the `component_mapping` to the `integrate_features` function. \\n\\nPlease note that the `integrate_features` function assumes that the project components are Python files and that the feature instructions are valid Python code. \\n\\nIf you need to integrate features into non-Python files or if the feature instructions are not valid Python code, you may need to modify the `integrate_features` function or use a different approach. \\n\\nAlso, the `integrate_features` function does'}\n",
      "*********************************************************************************************\n",
      "{'question': 'How to integrate the specified features into the project components?', 'result': ' You can use the `integrate_features` function from the `feature_integration.py` module, passing in the list of project components and the feature instructions as arguments. For example: `integrate_features(project_components=[\"component1.py\", \"component2.py\"], feature_instructions=\"import feature_module\\\\nfeature_module.do_something()\")`. This will append the feature instructions to the end of each project component file. \\n\\nNote: The `integrate_features` function does not return any value, it modifies the project components in-place. \\n\\nAlso, you can use the `map_features_to_components` function from the `feature_mapper.py` module to map feature requests to specific project components before integrating the features. \\n\\nFor example: `component_mapping = map_features_to_components(project_data=[{\"path\": \"component1.py\", \"content\": \"This is component 1\"}, {\"path\": \"component2.py\", \"content\": \"This is component 2\"}], feature_request=\"feature_request\")`. \\n\\nThen you can pass the `component_mapping` to the `integrate_features` function. \\n\\nPlease note that the `integrate_features` function assumes that the project components are Python files and that the feature instructions are valid Python code. \\n\\nIf you need to integrate features into non-Python files or if the feature instructions are not valid Python code, you may need to modify the `integrate_features` function or use a different approach. \\n\\nAlso, the `integrate_features` function does'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from chromadb import PersistentClient\n",
    "import torch\n",
    "\n",
    "# Load credentials from .env file\n",
    "load_dotenv()\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if not WATSONX_APIKEY or not PROJECT_ID:\n",
    "    raise ValueError(\"API key or Project ID is missing. Please check your .env file.\")\n",
    "\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding_method, temperature):\n",
    "    \"\"\"\n",
    "    Initializes and returns a WatsonxLLM instance with the specified parameters.\n",
    "    \"\"\"\n",
    "    return WatsonxLLM(\n",
    "        model_id=model_type,\n",
    "        url=\"https://eu-gb.ml.cloud.ibm.com\",\n",
    "        project_id=PROJECT_ID,\n",
    "        params={\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"min_new_tokens\": min_tokens,\n",
    "            \"decoding_method\": decoding_method,\n",
    "            \"temperature\": temperature,\n",
    "        },\n",
    "    )\n",
    "\n",
    "def answer_questions_from_dataframe(question, collection_name, persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Answers a question using a LangChain model and retrieves relevant documents from a Chroma collection.\n",
    "    \"\"\"\n",
    "    # Specify model parameters\n",
    "    model_type = \"meta-llama/llama-3-1-70b-instruct\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 100\n",
    "    decoding_method = \"greedy\"\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Initialize the WatsonxLLM model\n",
    "    model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding_method, temperature)\n",
    "\n",
    "    # Use the same embedding model as in create_embeddings_and_store for consistency\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_kwargs = {'device': device}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "    # Initialize Chroma Persistent Client\n",
    "    chroma_client = PersistentClient(path=persist_directory)\n",
    "\n",
    "    # Ensure the collection exists\n",
    "    if collection_name not in [col.name for col in chroma_client.list_collections()]:\n",
    "        raise ValueError(f\"Collection '{collection_name}' does not exist. Make sure it is created.\")\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "\n",
    "    # Use Chroma vectorstore with the given collection\n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=hf,\n",
    "        client=chroma_client,\n",
    "    )\n",
    "\n",
    "    # Create a retriever from the vectorstore\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # Build the RetrievalQA chain\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        input_key=\"question\"\n",
    "    )\n",
    "\n",
    "    # Run the chain with the question\n",
    "    response_text = chain.invoke({\"question\": question})\n",
    "\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text)\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# Example Usage\n",
    "question = \"How to integrate the specified features into the project components?\"\n",
    "\n",
    "# Ensure consistent settings\n",
    "persist_directory = \"./chroma_db\"\n",
    "collection_name = \"my_vector_collection\"\n",
    "\n",
    "response = answer_questions_from_dataframe(question, collection_name, persist_directory)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
