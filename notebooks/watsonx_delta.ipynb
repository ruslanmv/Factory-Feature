{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM...\n",
      "Done with LLM initialization.\n",
      "Step 1: Creating DataFrame from project...\n",
      "Done with Step 1.\n",
      "Step 2: Enhancing metadata...\n",
      "Done with Step 2.\n",
      "Step 3: Creating vector database...\n",
      "Done with Step 3.\n",
      "Step 4-6: Inferring modifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model initialized.\n",
      "Connected to ChromaDB at ./chroma_db\n",
      "Using collection: project_vectors\n",
      "Retriever initialized.\n",
      "RetrievalQA chain initialized.\n",
      "Prompt constructed for LLM inference.\n",
      "Inference completed successfully.\n",
      "Done with Step 4-6.\n",
      "Step 7-9: Updating project tree...\n",
      "File created: ./project_new/update_1.txt\n",
      "File created: ./project_new/update_2.txt\n",
      "Done with Step 7-9. Project tree updated successfully.\n",
      "Project successfully generated at ./project_new\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from chromadb import PersistentClient\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "def initialize_llm():\n",
    "    \"\"\"\n",
    "    Initialize the WatsonxLLM model.\n",
    "    \"\"\"\n",
    "    print(\"Initializing LLM...\")\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        url=\"https://eu-gb.ml.cloud.ibm.com\",\n",
    "        project_id=PROJECT_ID,\n",
    "        params={\n",
    "            \"max_new_tokens\": 300,\n",
    "            \"min_new_tokens\": 50,\n",
    "            \"decoding_method\": \"greedy\",\n",
    "            \"temperature\": 0.7,\n",
    "        },\n",
    "    )\n",
    "    print(\"Done with LLM initialization.\")\n",
    "    return llm\n",
    "\n",
    "def create_dataframe_from_project(project_path):\n",
    "    \"\"\"\n",
    "    Step 1: Create a DataFrame of the project files.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Creating DataFrame from project...\")\n",
    "    data = []\n",
    "    for root, _, files in os.walk(project_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                readable = True\n",
    "            except Exception:\n",
    "                content = \"\"\n",
    "                readable = False\n",
    "            data.append({\n",
    "                \"path\": file_path,\n",
    "                \"content\": content,\n",
    "                \"readable\": \"Yes\" if readable else \"No\",\n",
    "                \"extension\": os.path.splitext(file)[-1],\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Done with Step 1.\")\n",
    "    return df\n",
    "\n",
    "def enhance_metadata(df, llm):\n",
    "    \"\"\"\n",
    "    Step 2: Enhance metadata with descriptions generated by LLM.\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Enhancing metadata...\")\n",
    "    \n",
    "    def generate_description(row):\n",
    "        if row['readable'] == \"No\":\n",
    "            return \"Unreadable file.\"\n",
    "        prompt = f\"Describe the purpose and content of this file:\\n{row['content'][:500]}\"\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except Exception as e:\n",
    "            return f\"Error generating description: {str(e)}\"\n",
    "    \n",
    "    df['description'] = df.apply(generate_description, axis=1)\n",
    "    print(\"Done with Step 2.\")\n",
    "    return df\n",
    "\n",
    "def create_vector_database(df, db_path):\n",
    "    \"\"\"\n",
    "    Step 3: Create a vector database from the DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Step 3: Creating vector database...\")\n",
    "    chroma_client = PersistentClient(path=db_path)\n",
    "    collection_name = \"project_vectors\"\n",
    "    if collection_name in [col.name for col in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "    collection = chroma_client.create_collection(name=collection_name)\n",
    "    \n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "    )\n",
    "    documents = df['description'].tolist()\n",
    "    ids = df.index.astype(str).tolist()\n",
    "    embeddings = hf.embed_documents(documents)\n",
    "    collection.add(documents=documents, ids=ids, embeddings=embeddings)\n",
    "    \n",
    "    print(\"Done with Step 3.\")\n",
    "    return collection\n",
    "\n",
    "def infer_modifications(llm, collection_name, persist_directory, feature_instructions):\n",
    "    \"\"\"\n",
    "    Step 4-6: Infer project modifications based on instructions.\n",
    "\n",
    "    Args:\n",
    "        llm: The WatsonxLLM instance for inference.\n",
    "        collection_name: Name of the Chroma collection to use.\n",
    "        persist_directory: Directory where the Chroma database is stored.\n",
    "        feature_instructions: Instructions describing the modifications to be made.\n",
    "\n",
    "    Returns:\n",
    "        Response from the LLM with the inferred modifications and additions.\n",
    "    \"\"\"\n",
    "    print(\"Step 4-6: Inferring modifications...\")\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "        encode_kwargs={'normalize_embeddings': True},\n",
    "    )\n",
    "    print(\"Embedding model initialized.\")\n",
    "\n",
    "    # Initialize the Chroma Persistent Client\n",
    "    chroma_client = PersistentClient(path=persist_directory)\n",
    "    print(f\"Connected to ChromaDB at {persist_directory}\")\n",
    "\n",
    "    # Check if the collection exists\n",
    "    if collection_name not in [col.name for col in chroma_client.list_collections()]:\n",
    "        raise ValueError(f\"Collection '{collection_name}' does not exist. Ensure it is created before calling this function.\")\n",
    "\n",
    "    # Get the existing collection\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "    print(f\"Using collection: {collection_name}\")\n",
    "\n",
    "    # Initialize the vector store with the Chroma client and embedding function\n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=hf,\n",
    "        client=chroma_client,  # Use PersistentClient directly\n",
    "    )\n",
    "\n",
    "    # Create a retriever from the vector store\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    print(\"Retriever initialized.\")\n",
    "\n",
    "    # Build the RetrievalQA chain\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        input_key=\"question\",\n",
    "    )\n",
    "    print(\"RetrievalQA chain initialized.\")\n",
    "\n",
    "    # Construct the prompt for LLM inference\n",
    "    prompt = (\n",
    "        f\"Given the following project descriptions and instructions:\\n\\n{feature_instructions}\\n\\n\"\n",
    "        \"Based on the current project components, provide the following:\\n\"\n",
    "        \"1. Suggested modifications to existing components.\\n\"\n",
    "        \"2. New components or features to add.\\n\"\n",
    "        \"3. A brief explanation of why these changes are necessary.\\n\"\n",
    "    )\n",
    "    print(\"Prompt constructed for LLM inference.\")\n",
    "\n",
    "    # Run the RetrievalQA chain\n",
    "    try:\n",
    "        response = chain.invoke({\"question\": prompt})\n",
    "        print(\"Inference completed successfully.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during LLM inference: {str(e)}\")\n",
    "\n",
    "    print(\"Done with Step 4-6.\")\n",
    "    return response\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "def update_project_tree(modifications, new_project_path):\n",
    "    \"\"\"\n",
    "    Step 7-9: Apply modifications to create a new project.\n",
    "\n",
    "    Args:\n",
    "        modifications (list): List of modifications, where each item can be a string or a dictionary.\n",
    "        new_project_path (str): The root directory for the new project.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Step 7-9: Updating project tree...\")\n",
    "\n",
    "    # Ensure the base directory for the new project exists\n",
    "    os.makedirs(new_project_path, exist_ok=True)\n",
    "\n",
    "    for index, item in enumerate(modifications):\n",
    "        # Check if the item is a string or a dictionary\n",
    "        if isinstance(item, str):\n",
    "            # Treat string as a direct project-wide update (e.g., a general suggestion)\n",
    "            relative_path = f\"update_{index + 1}.txt\"  # Save it as a text file for reference\n",
    "            content = item\n",
    "        elif isinstance(item, dict):\n",
    "            # Treat dictionary as a specific file update\n",
    "            relative_path = item.get('path')\n",
    "            content = item.get('content', '')\n",
    "        else:\n",
    "            print(f\"Warning: Skipping modification #{index + 1} due to invalid type: {type(item)}\")\n",
    "            continue\n",
    "\n",
    "        if not relative_path:\n",
    "            print(f\"Warning: Skipping modification #{index + 1} due to missing 'path'.\")\n",
    "            continue\n",
    "\n",
    "        # Create the full path for the new file\n",
    "        file_path = os.path.join(new_project_path, relative_path)\n",
    "\n",
    "        try:\n",
    "            # Ensure the directory structure exists\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "            # Write the content to the file\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "\n",
    "            print(f\"File created: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating file '{file_path}': {e}\")\n",
    "            continue\n",
    "\n",
    "    print(\"Done with Step 7-9. Project tree updated successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "def generate_project(old_project_path, new_project_path, feature_instructions):\n",
    "    \"\"\"\n",
    "    Main function to generate a new project based on feature instructions.\n",
    "    \"\"\"\n",
    "    # Initialize LLM\n",
    "    llm = initialize_llm()\n",
    "    \n",
    "    # Step 1: Create a DataFrame of the project\n",
    "    df = create_dataframe_from_project(old_project_path)\n",
    "    \n",
    "    # Step 2: Enhance metadata\n",
    "    df = enhance_metadata(df, llm)\n",
    "    df.to_pickle(\"enhanced_project.pkl\")\n",
    "    \n",
    "    # Step 3: Create a vector database\n",
    "    collection = create_vector_database(df, \"./chroma_db\")\n",
    "    collection_name = \"project_vectors\"\n",
    "    persist_directory=\"./chroma_db\"\n",
    "    # Step 4-6: Infer modifications\n",
    "   \n",
    "    modifications = infer_modifications(llm, collection_name, persist_directory, feature_instructions)\n",
    "    # Step 7-9: Update the project tree and create new project\n",
    "    update_project_tree(modifications, new_project_path)\n",
    "    print(f\"Project successfully generated at {new_project_path}\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    old_path = \"./project_old\"\n",
    "    new_path = \"./project_new\"\n",
    "    instructions = \"Add logging to all major modules and improve error handling.\"\n",
    "    generate_project(old_path, new_path, instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM...\n",
      "LLM initialized successfully.\n",
      "Step 1: Creating DataFrame from project files...\n",
      "Project DataFrame created.\n",
      "Step 2: Enhancing metadata...\n",
      "Metadata enhancement completed.\n",
      "Step 3: Creating vector database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Blog/Building-LLM-from-Scratch-in-Python/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully.\n",
      "Inferring modifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_197689/3649362611.py:122: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(\n",
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating project tree...\n",
      "New project generated at ./project_new.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from chromadb import PersistentClient\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "\n",
    "def initialize_llm():\n",
    "    \"\"\"\n",
    "    Initialize the WatsonxLLM model.\n",
    "    \"\"\"\n",
    "    print(\"Initializing LLM...\")\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        url=\"https://eu-gb.ml.cloud.ibm.com\",\n",
    "        project_id=PROJECT_ID,\n",
    "        params={\n",
    "            \"max_new_tokens\": 300,\n",
    "            \"min_new_tokens\": 50,\n",
    "            \"decoding_method\": \"greedy\",\n",
    "            \"temperature\": 0.7,\n",
    "        },\n",
    "    )\n",
    "    print(\"LLM initialized successfully.\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "def create_dataframe_from_project(project_path):\n",
    "    \"\"\"\n",
    "    Step 1: Create a DataFrame of the project files.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Creating DataFrame from project files...\")\n",
    "    data = []\n",
    "    for root, _, files in os.walk(project_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                readable = True\n",
    "            except Exception:\n",
    "                content = \"\"\n",
    "                readable = False\n",
    "            data.append({\n",
    "                \"path\": file_path,\n",
    "                \"content\": content,\n",
    "                \"readable\": \"Yes\" if readable else \"No\",\n",
    "                \"extension\": os.path.splitext(file)[-1],\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Project DataFrame created.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def enhance_metadata(df, llm):\n",
    "    \"\"\"\n",
    "    Step 2: Enhance metadata with descriptions generated by LLM.\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Enhancing metadata...\")\n",
    "    \n",
    "    def generate_description(row):\n",
    "        if row['readable'] == \"No\":\n",
    "            return \"Unreadable file.\"\n",
    "        prompt = f\"Describe the purpose and content of this file:\\n{row['content'][:500]}\"\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except Exception as e:\n",
    "            return f\"Error generating description: {str(e)}\"\n",
    "    \n",
    "    df['description'] = df.apply(generate_description, axis=1)\n",
    "    print(\"Metadata enhancement completed.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_vector_database(df, db_path):\n",
    "    \"\"\"\n",
    "    Step 3: Create a vector database from the DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Step 3: Creating vector database...\")\n",
    "    chroma_client = PersistentClient(path=db_path)\n",
    "    collection_name = \"project_vectors\"\n",
    "    if collection_name in [col.name for col in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "    collection = chroma_client.create_collection(name=collection_name)\n",
    "    \n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "    )\n",
    "    documents = df['description'].tolist()\n",
    "    ids = df.index.astype(str).tolist()\n",
    "    embeddings = hf.embed_documents(documents)\n",
    "    collection.add(documents=documents, ids=ids, embeddings=embeddings)\n",
    "    \n",
    "    print(\"Vector database created successfully.\")\n",
    "    return collection\n",
    "\n",
    "\n",
    "def infer_modifications(llm, collection_name, persist_directory, feature_instructions):\n",
    "    \"\"\"\n",
    "    Step 4-6: Infer modifications using LLM and vector database.\n",
    "    \"\"\"\n",
    "    print(\"Inferring modifications...\")\n",
    "    \n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    )\n",
    "    chroma_client = PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=hf,\n",
    "        client=chroma_client,\n",
    "    )\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        input_key=\"question\",\n",
    "    )\n",
    "    prompt = (\n",
    "        f\"Based on these project components and the instruction:\\n{feature_instructions}\\n\\n\"\n",
    "        \"Provide:\\n\"\n",
    "        \"1. Components to modify\\n\"\n",
    "        \"2. New components to add\\n\"\n",
    "        \"3. Reasoning for each change\\n\"\n",
    "    )\n",
    "    response = chain.invoke({\"question\": prompt})\n",
    "    return response\n",
    "\n",
    "\n",
    "def update_project_tree(modifications, old_project_path, new_project_path):\n",
    "    \"\"\"\n",
    "    Step 7-9: Update the project tree with modifications.\n",
    "    \"\"\"\n",
    "    print(\"Updating project tree...\")\n",
    "    shutil.copytree(old_project_path, new_project_path, dirs_exist_ok=True)\n",
    "    for mod in modifications:\n",
    "        if isinstance(mod, dict) and 'path' in mod and 'content' in mod:\n",
    "            new_file_path = os.path.join(new_project_path, mod['path'])\n",
    "            os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(mod['content'])\n",
    "    print(f\"New project generated at {new_project_path}.\")\n",
    "\n",
    "\n",
    "def generate_project(old_project_path, new_project_path, feature_instructions):\n",
    "    \"\"\"\n",
    "    Main function to generate the new project.\n",
    "    \"\"\"\n",
    "    llm = initialize_llm()\n",
    "    df = create_dataframe_from_project(old_project_path)\n",
    "    df = enhance_metadata(df, llm)\n",
    "    df.to_pickle(\"enhanced_project.pkl\")\n",
    "    create_vector_database(df, \"./chroma_db\")\n",
    "    modifications = infer_modifications(\n",
    "        llm, \"project_vectors\", \"./chroma_db\", feature_instructions\n",
    "    )\n",
    "    update_project_tree(modifications, old_project_path, new_project_path)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    old_path = \"./project_old\"\n",
    "    new_path = \"./project_new\"\n",
    "    feature_request = \"Add logging to all modules and create a utils module.\"\n",
    "    generate_project(old_path, new_path, feature_request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM...\n",
      "LLM initialized successfully.\n",
      "Step 1: Creating DataFrame from project files...\n",
      "Project DataFrame created.\n",
      "Step 2: Enhancing metadata...\n",
      "Metadata enhancement completed.\n",
      "Step 3: Creating vector database...\n",
      "Vector database created successfully.\n",
      "Inferring modifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating project tree...\n",
      "New project generated at ./project_new.\n",
      "Generating report...\n",
      "Report generated and saved to ./project_new/modification_report.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from chromadb import PersistentClient\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "\n",
    "def initialize_llm():\n",
    "    \"\"\"\n",
    "    Initialize the WatsonxLLM model.\n",
    "    \"\"\"\n",
    "    print(\"Initializing LLM...\")\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=\"mistralai/mixtral-8x7b-instruct-v01\",\n",
    "        url=\"https://eu-gb.ml.cloud.ibm.com\",\n",
    "        project_id=PROJECT_ID,\n",
    "        params={\n",
    "            \"max_new_tokens\": 300,\n",
    "            \"min_new_tokens\": 50,\n",
    "            \"decoding_method\": \"greedy\",\n",
    "            \"temperature\": 0.7,\n",
    "        },\n",
    "    )\n",
    "    print(\"LLM initialized successfully.\")\n",
    "    return llm\n",
    "\n",
    "\n",
    "def create_dataframe_from_project(project_path):\n",
    "    \"\"\"\n",
    "    Step 1: Create a DataFrame of the project files.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Creating DataFrame from project files...\")\n",
    "    data = []\n",
    "    for root, _, files in os.walk(project_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                readable = True\n",
    "            except Exception:\n",
    "                content = \"\"\n",
    "                readable = False\n",
    "            data.append({\n",
    "                \"path\": file_path,\n",
    "                \"content\": content,\n",
    "                \"readable\": \"Yes\" if readable else \"No\",\n",
    "                \"extension\": os.path.splitext(file)[-1],\n",
    "            })\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Project DataFrame created.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def enhance_metadata(df, llm):\n",
    "    \"\"\"\n",
    "    Step 2: Enhance metadata with descriptions generated by LLM.\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Enhancing metadata...\")\n",
    "    \n",
    "    def generate_description(row):\n",
    "        if row['readable'] == \"No\":\n",
    "            return \"Unreadable file.\"\n",
    "        prompt = f\"Please generate a small description about the content of this file  \\n{row['path'][:500]} and what it does with content:\\n{row['content'][:500]}\"\n",
    "        try:\n",
    "            return llm.invoke(prompt)\n",
    "        except Exception as e:\n",
    "            return f\"Error generating description: {str(e)}\"\n",
    "    \n",
    "    df['description'] = df.apply(generate_description, axis=1)\n",
    "    print(\"Metadata enhancement completed.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_vector_database(df, db_path):\n",
    "    \"\"\"\n",
    "    Step 3: Create a vector database from the DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Step 3: Creating vector database...\")\n",
    "    chroma_client = PersistentClient(path=db_path)\n",
    "    collection_name = \"project_vectors\"\n",
    "    if collection_name in [col.name for col in chroma_client.list_collections()]:\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "    collection = chroma_client.create_collection(name=collection_name)\n",
    "    \n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"}\n",
    "    )\n",
    "    documents = df['description'].tolist()\n",
    "    ids = df.index.astype(str).tolist()\n",
    "    embeddings = hf.embed_documents(documents)\n",
    "    collection.add(documents=documents, ids=ids, embeddings=embeddings)\n",
    "    \n",
    "    print(\"Vector database created successfully.\")\n",
    "    return collection\n",
    "\n",
    "\n",
    "def infer_modifications(llm, collection_name, persist_directory, feature_instructions):\n",
    "    \"\"\"\n",
    "    Step 4-6: Infer modifications using LLM and vector database.\n",
    "    \"\"\"\n",
    "    print(\"Inferring modifications...\")\n",
    "    \n",
    "    hf = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    )\n",
    "    chroma_client = PersistentClient(path=persist_directory)\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=hf,\n",
    "        client=chroma_client,\n",
    "    )\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        input_key=\"question\",\n",
    "    )\n",
    "    prompt = (\n",
    "        f\"Based on these project components and the instruction:\\n{feature_instructions}\\n\\n\"\n",
    "        \"Provide:\\n\"\n",
    "        \"1. Components to modify\\n\"\n",
    "        \"2. New components to add\\n\"\n",
    "        \"3. Reasoning for each change\\n\"\n",
    "    )\n",
    "    response = chain.invoke({\"question\": prompt})\n",
    "    return response\n",
    "\n",
    "\n",
    "def update_project_tree(modifications, old_project_path, new_project_path):\n",
    "    \"\"\"\n",
    "    Step 7-9: Update the project tree with modifications and return report data.\n",
    "    \"\"\"\n",
    "    print(\"Updating project tree...\")\n",
    "    shutil.copytree(old_project_path, new_project_path, dirs_exist_ok=True)\n",
    "    report = []\n",
    "\n",
    "    for mod in modifications:\n",
    "        if isinstance(mod, dict) and 'path' in mod and 'content' in mod:\n",
    "            new_file_path = os.path.join(new_project_path, mod['path'])\n",
    "            old_file_path = os.path.join(old_project_path, mod['path'])\n",
    "            os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n",
    "\n",
    "            # Write the new content\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(mod['content'])\n",
    "\n",
    "            # Check if the file existed in the old project\n",
    "            exists_in_old = os.path.exists(old_file_path)\n",
    "            report.append({\n",
    "                \"old_path\": old_file_path if exists_in_old else None,\n",
    "                \"new_path\": new_file_path,\n",
    "                \"status\": \"Modified\" if exists_in_old else \"Created\"\n",
    "            })\n",
    "\n",
    "    print(f\"New project generated at {new_project_path}.\")\n",
    "    return report\n",
    "\n",
    "\n",
    "def generate_report(report, output_path):\n",
    "    \"\"\"\n",
    "    Step 10: Generate a report of modifications and save to a file.\n",
    "    \"\"\"\n",
    "    print(\"Generating report...\")\n",
    "    report_df = pd.DataFrame(report)\n",
    "    report_file = os.path.join(output_path, \"modification_report.csv\")\n",
    "    report_df.to_csv(report_file, index=False)\n",
    "    print(f\"Report generated and saved to {report_file}.\")\n",
    "\n",
    "\n",
    "def generate_project(old_project_path, new_project_path, feature_instructions):\n",
    "    \"\"\"\n",
    "    Main function to generate the new project.\n",
    "    \"\"\"\n",
    "    llm = initialize_llm()\n",
    "    df = create_dataframe_from_project(old_project_path)\n",
    "    df = enhance_metadata(df, llm)\n",
    "    df.to_pickle(\"enhanced_project.pkl\")\n",
    "    create_vector_database(df, \"./chroma_db\")\n",
    "    modifications = infer_modifications(\n",
    "        llm, \"project_vectors\", \"./chroma_db\", feature_instructions\n",
    "    )\n",
    "    report = update_project_tree(modifications, old_project_path, new_project_path)\n",
    "    generate_report(report, new_project_path)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    old_path = \"./project_old\"\n",
    "    new_path = \"./project_new\"\n",
    "    feature_request = \"Add logging to all modules and create a utils module.\"\n",
    "    generate_project(old_path, new_path, feature_request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LLM...\n",
      "LLM initialized successfully.\n",
      "Step 1: Creating DataFrame from project files...\n",
      "Project DataFrame created.\n"
     ]
    }
   ],
   "source": [
    "old_project_path = \"./project_old\"\n",
    "new_path = \"./project_new\"\n",
    "feature_instructions = \"Add logging to all modules and create a utils module.\"\n",
    "llm = initialize_llm()\n",
    "df = create_dataframe_from_project(old_project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>readable</th>\n",
       "      <th>extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./project_old/app.py</td>\n",
       "      <td>from utils.helpers import greet\\n\\ndef main():...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>.py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./project_old/requirements.txt</td>\n",
       "      <td># Python dependencies\\nflask\\n</td>\n",
       "      <td>Yes</td>\n",
       "      <td>.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./project_old/utils/helpers.py</td>\n",
       "      <td>def greet(name):\\n    \"\"\"\\n    Returns a greet...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>.py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             path  \\\n",
       "0            ./project_old/app.py   \n",
       "1  ./project_old/requirements.txt   \n",
       "2  ./project_old/utils/helpers.py   \n",
       "\n",
       "                                             content readable extension  \n",
       "0  from utils.helpers import greet\\n\\ndef main():...      Yes       .py  \n",
       "1                     # Python dependencies\\nflask\\n      Yes      .txt  \n",
       "2  def greet(name):\\n    \"\"\"\\n    Returns a greet...      Yes       .py  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Enhancing metadata...\n",
      "Metadata enhancement completed.\n"
     ]
    }
   ],
   "source": [
    "df = enhance_metadata(df, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\nThis file path  ./project_old/app.py is the main entry point for the application. It imports a greet function from a utils.helpers module and uses it in the main function to greet the user. The user is prompted to enter their name, and the greet function is called with the entered name as an argument. The result is then printed to the console.',\n",
       "       'flask_sqlalchemy\\nflask_migrate\\nflask_login\\nflask_wtf\\nflask_bootstrap\\nflask_socketio\\ngevent\\ngevent-websocket\\npsycopg2\\ngunicorn\\n\\nThis file path ./project_old/requirements.txt is a list of Python dependencies for a Flask web application. Each line in the file specifies a Python package that is required to run the application. The packages are used for various purposes such as database management, user authentication, form handling, and real-time communication.\\n\\nHere is a brief description of each package:\\n\\n* flask: A lightweight web framework for building web applications in Python.\\n* flask_sqlalchemy: An extension for Flask that provides integration with SQLAlchemy, a popular Object-Relational Mapping (ORM) library for Python.\\n* flask_migrate: An extension for Flask that provides database schema migrations using Alembic.\\n* flask_login: An extension for Flask that provides user authentication and session management.\\n* flask_wtf: An extension for Flask that provides integration with the WTForms library for handling forms.\\n* flask_bootstrap: An extension for Flask that provides integration with the Bootstrap CSS framework.\\n* flask_socketio: An extension for Flask that provides real-time',\n",
       "       '\\ndef get_file_extension(filename):\\n    \"\"\"\\n    Returns the file extension of a given filename.\\n\\n    Args:\\n        filename: The name of the file.\\n\\n    Returns:\\n        The file extension as a string.\\n    \"\"\"\\n    return filename.split(\".\")[-1]\\n\\ndef is_valid_file_type(filename, allowed_extensions):\\n    \"\"\"\\n    Checks if a given filename has a valid file extension.\\n\\n    Args:\\n        filename: The name of the file.\\n        allowed_extensions: A list of allowed file extensions.\\n\\n    Returns:\\n        True if the file extension is valid, False otherwise.\\n    \"\"\"\\n    return get_file_extension(filename) in allowed_extensions\\n\\ndef read_file(filepath):\\n    \"\"\"\\n    Reads the contents of a file and returns it as a string.\\n\\n    Args:\\n        filepath: The path to the file.\\n\\n    Returns:\\n        The contents of the file as a string.\\n    \"\"\"\\n    with open(filepath, \"r\") as file:\\n        return file.read()\\n\\ndef write_file(filepath, content):\\n    \"\"\"\\n    Writes the provided content to a file.\\n\\n    Args:\\n        filepath: The path to the file.\\n        content: The content to'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"description\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"enhanced_project.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Creating vector database...\n",
      "Vector database created successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Collection(name=project_vectors)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vector_database(df, \"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring modifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "modifications = infer_modifications(\n",
    "llm, \"project_vectors\", \"./chroma_db\", feature_instructions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Based on these project components and the instruction:\\nAdd logging to all modules and create a utils module.\\n\\nProvide:\\n1. Components to modify\\n2. New components to add\\n3. Reasoning for each change\\n',\n",
       " 'result': '\\n\\n1. Components to modify:\\n   - All modules: Add logging functionality to all modules. This can be done by importing the logging module and adding logging statements throughout the code.\\n\\n2. New components to add:\\n   - utils module: Create a new module called utils.py and add the get_file_extension, is_valid_file_type, read_file, and write_file functions to this module. This will help to centralize and reuse common functionality across the application.\\n\\n3. Reasoning for each change:\\n   - Adding logging to all modules: Logging is an important tool for debugging and monitoring applications. By adding logging to all modules, we can easily track the flow of execution and identify any errors or issues that may arise.\\n   - Creating a utils module: By creating a utils module, we can centralize and reuse common functionality across the application. This will help to reduce code duplication and improve maintainability. Additionally, it will make it easier to share code between different modules and applications.'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Add logging to all modules and refactor utilities to improve performance.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from chromadb import PersistentClient\n",
    "import torch\n",
    "\n",
    "# Load credentials from .env file\n",
    "load_dotenv()\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if not WATSONX_APIKEY or not PROJECT_ID:\n",
    "    raise ValueError(\"API key or Project ID is missing. Please check your .env file.\")\n",
    "\n",
    "\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding_method, temperature):\n",
    "    \"\"\"\n",
    "    Initializes and returns a WatsonxLLM instance with the specified parameters.\n",
    "    \"\"\"\n",
    "    print(\"Initializing WatsonxLLM model...\")\n",
    "    return WatsonxLLM(\n",
    "        model_id=model_type,\n",
    "        url=\"https://eu-gb.ml.cloud.ibm.com\",\n",
    "        project_id=PROJECT_ID,\n",
    "        params={\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"min_new_tokens\": min_tokens,\n",
    "            \"decoding_method\": decoding_method,\n",
    "            \"temperature\": temperature,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def infer_modifications(\n",
    "    collection_name, persist_directory, feature_instructions, output_json_path\n",
    "):\n",
    "    \"\"\"\n",
    "    Step 4-6: Infer modifications using WatsonxLLM and vector database.\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Name of the ChromaDB collection.\n",
    "        persist_directory (str): Directory path for ChromaDB persistence.\n",
    "        feature_instructions (str): Feature request instructions for modifications.\n",
    "        output_json_path (str): Path to save the JSON report of modifications.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: A structured list of modifications, including paths and actions.\n",
    "    \"\"\"\n",
    "    print(\"Inferring modifications with WatsonxLLM...\")\n",
    "\n",
    "    # WatsonxLLM configuration\n",
    "    model_type = \"meta-llama/llama-3-1-70b-instruct\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 100\n",
    "    decoding_method = \"greedy\"\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Initialize the WatsonxLLM model\n",
    "    llm = get_lang_chain_model(\n",
    "        model_type, max_tokens, min_tokens, decoding_method, temperature\n",
    "    )\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\"device\": device},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    # Initialize ChromaDB Persistent Client\n",
    "    chroma_client = PersistentClient(path=persist_directory)\n",
    "    if collection_name not in [col.name for col in chroma_client.list_collections()]:\n",
    "        raise ValueError(f\"Collection '{collection_name}' does not exist. Ensure it is created.\")\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "\n",
    "    # Use Chroma vectorstore with the given collection\n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=hf_embeddings,\n",
    "        client=chroma_client,\n",
    "    )\n",
    "\n",
    "    # Create a retriever from the vectorstore\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # Build the RetrievalQA chain\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        input_key=\"question\",\n",
    "    )\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = (\n",
    "        f\"Based on the project descriptions and these instructions:\\n{feature_instructions}\\n\\n\"\n",
    "        \"Provide a list of:\\n\"\n",
    "        \"1. Paths to files that should be modified.\\n\"\n",
    "        \"2. The specific modifications required for each file.\\n\"\n",
    "        \"3. Any new files or directories to be created and their purposes.\\n\\n\"\n",
    "        \"Return the output in JSON format for further analysis.\"\n",
    "        '''For example:\n",
    "        \"Generate a JSON response with the following structure:\\n\"\n",
    "        [\n",
    "        {\"path\": \"./src/main.py\", \"modification\": \"Add logging\"},\n",
    "        {\"path\": \"./utils/helpers.py\", \"modification\": \"Create the logins function\"},\n",
    "        {\"path\": \"requirements.txt\", \"modification\": \"Add Python requirements compatible with pip install\"}\n",
    "        ]\n",
    "        '''\n",
    " \n",
    "    )\n",
    "\n",
    "    # Run the chain with the prompt\n",
    "    response_text = chain.invoke({\"question\": prompt})\n",
    "\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text)\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "\n",
    "    return response_text\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferring modifications with WatsonxLLM...\n",
      "Initializing WatsonxLLM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- Generated response -----------------------------------\n",
      "{'question': 'Based on the project descriptions and these instructions:\\nAdd logging to all modules and refactor utilities to improve performance.\\n\\nProvide a list of:\\n1. Paths to files that should be modified.\\n2. The specific modifications required for each file.\\n3. Any new files or directories to be created and their purposes.\\n\\nReturn the output in JSON format for further analysis.For example:\\n        \"Generate a JSON response with the following structure:\\n\"\\n        [\\n        {\"path\": \"./src/main.py\", \"modification\": \"Add logging\"},\\n        {\"path\": \"./utils/helpers.py\", \"modification\": \"Create the logins function\"},\\n        {\"path\": \"requirements.txt\", \"modification\": \"Add Python requirements compatible with pip install\"}\\n        ]\\n        ', 'result': ' \\n[\\n{\"path\": \"./project_old/requirements.txt\", \"modification\": \"Add Python requirements compatible with pip install\"},\\n{\"path\": \"./project_old/app.py\", \"modification\": \"Add logging\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Add logging and refactor utilities to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Create the logins function\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the greet function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the get_file_extension function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the is_valid_file_type function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the read_file function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the write_file function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the get_file_extension function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the is_valid_file_type function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the read_file function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the write_file function'}\n",
      "*********************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "feature_instructions = (\n",
    "    \"Add logging to all modules and refactor utilities to improve performance.\"\n",
    ")\n",
    "collection_name = \"project_vectors\"\n",
    "persist_directory = \"./chroma_db\"\n",
    "output_json_path = \"./modifications.json\"\n",
    "\n",
    "modifications = infer_modifications(\n",
    "    collection_name, persist_directory, feature_instructions, output_json_path\n",
    ")\n",
    "\n",
    "#print(\"Modifications:\")\n",
    "#print(json.dumps(modifications, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Based on the project descriptions and these instructions:\\nAdd logging to all modules and refactor utilities to improve performance.\\n\\nProvide a list of:\\n1. Paths to files that should be modified.\\n2. The specific modifications required for each file.\\n3. Any new files or directories to be created and their purposes.\\n\\nReturn the output in JSON format for further analysis.For example:\\n        \"Generate a JSON response with the following structure:\\n\"\\n        [\\n        {\"path\": \"./src/main.py\", \"modification\": \"Add logging\"},\\n        {\"path\": \"./utils/helpers.py\", \"modification\": \"Create the logins function\"},\\n        {\"path\": \"requirements.txt\", \"modification\": \"Add Python requirements compatible with pip install\"}\\n        ]\\n        ',\n",
       " 'result': ' \\n[\\n{\"path\": \"./project_old/requirements.txt\", \"modification\": \"Add Python requirements compatible with pip install\"},\\n{\"path\": \"./project_old/app.py\", \"modification\": \"Add logging\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Add logging and refactor utilities to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Create the logins function\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the greet function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the get_file_extension function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the is_valid_file_type function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the read_file function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the write_file function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the get_file_extension function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the is_valid_file_type function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the read_file function to improve performance\"},\\n{\"path\": \"./project_old/utils/helpers.py\", \"modification\": \"Refactor the write_file function'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to parse LLM response into JSON: Could not extract JSON response from the LLM output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not extract JSON response from the LLM output.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Could not extract JSON response from the LLM output.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse is not in the expected JSON format.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to parse LLM response into JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to parse LLM response into JSON: Could not extract JSON response from the LLM output."
     ]
    }
   ],
   "source": [
    "# Extract relevant content\n",
    "try:\n",
    "    if isinstance(response, dict) and \"question\" in response:\n",
    "        # Extract and clean the response text\n",
    "        response_text = response.get(\"question\", \"\")\n",
    "        start_index = response_text.find(\"[\")\n",
    "        end_index = response_text.rfind(\"]\") + 1\n",
    "\n",
    "        if start_index != -1 and end_index != -1:\n",
    "            relevant_response = response_text[start_index:end_index]\n",
    "        else:\n",
    "            raise ValueError(\"Could not extract JSON response from the LLM output.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected response format from LLM.\")\n",
    "\n",
    "    # Parse the JSON content\n",
    "    modifications = json.loads(relevant_response)\n",
    "\n",
    "    if isinstance(modifications, list):\n",
    "        # Save the modifications to a JSON file\n",
    "        with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(modifications, f, indent=4)\n",
    "        print(f\"Modifications saved to {output_json_path}.\")\n",
    "    else:\n",
    "        raise ValueError(\"Response is not in the expected JSON format.\")\n",
    "\n",
    "except (json.JSONDecodeError, ValueError) as e:\n",
    "    raise RuntimeError(f\"Failed to parse LLM response into JSON: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Based on these project components and the instruction:\\nAdd logging to all modules and create a utils module.\\n\\nProvide:\\n1. Components to modify\\n2. New components to add\\n3. Reasoning for each change\\n', 'result': '1. psycopg2\\n2. utils\\n3. Add logging to all modules and create a utils module.\\n\\n\\n\\n psycopg2: psycopg2 is a Python PostgreSQL database adapter. psycopg2 is a fork of the PostgreSQL adapter psycopg, and is the most actively maintained and most widely used PostgreSQL adapter for Python. psycopg2 is a drop-in replacement for psycopg, and can be used with any Python code that uses psycopg. psycopg2 is a mature and stable project, and has been tested with Python 2.6, 2.7, and 3.3 through 3.7. psycopg2 is a mature and stable project, and has been tested with Python 2.6, 2.7, and 3.3 through 3.7. psycopg2 is a mature and stable project, and has been tested with Python 2.6, 2.7, and 3.3 through 3.7. psycopg2 is a mature and stable project, and has been tested with Python 2.6, 2.7, and 3.3 through 3.7. psycopg2 is a mature and stable project, and has been tested with Python 2.6, 2.7, and 3.3 through 3.7. psycopg2 is a mature and stable project, and has been tested with Python 2.6,'}\n"
     ]
    }
   ],
   "source": [
    "print(modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_first_five_documents(collection):\n",
    "    \"\"\"\n",
    "    Reads the first 5 documents from the given ChromaDB collection and displays them.\n",
    "\n",
    "    Args:\n",
    "        collection (chromadb.Collection): The ChromaDB collection to read from.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Retrieve all documents from the collection\n",
    "    results = collection.get()\n",
    "    \n",
    "    # Extract documents and IDs\n",
    "    documents = results.get(\"documents\", [])\n",
    "    ids = results.get(\"ids\", [])\n",
    "    metadatas = results.get(\"metadatas\", [])\n",
    "\n",
    "    # Display the first 5 documents\n",
    "    print(f\"Displaying the first {min(len(documents), 5)} documents:\")\n",
    "    for i in range(min(len(documents), 5)):\n",
    "        print(f\"Document ID: {ids[i]}\")\n",
    "        print(f\"Content: {documents[i]}\")\n",
    "        if metadatas:\n",
    "            print(f\"Metadata: {metadatas[i]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Usage Example\n",
    "display_first_five_documents(collection)def display_first_five_documents(collection):\n",
    "    \"\"\"\n",
    "    Reads the first 5 documents from the given ChromaDB collection and displays them.\n",
    "\n",
    "    Args:\n",
    "        collection (chromadb.Collection): The ChromaDB collection to read from.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Retrieve all documents from the collection\n",
    "    results = collection.get()\n",
    "    \n",
    "    # Extract documents and IDs\n",
    "    documents = results.get(\"documents\", [])\n",
    "    ids = results.get(\"ids\", [])\n",
    "    metadatas = results.get(\"metadatas\", [])\n",
    "\n",
    "    # Display the first 5 documents\n",
    "    print(f\"Displaying the first {min(len(documents), 5)} documents:\")\n",
    "    for i in range(min(len(documents), 5)):\n",
    "        print(f\"Document ID: {ids[i]}\")\n",
    "        print(f\"Content: {documents[i]}\")\n",
    "        if metadatas:\n",
    "            print(f\"Metadata: {metadatas[i]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Usage Example\n",
    "display_first_five_documents(collection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
