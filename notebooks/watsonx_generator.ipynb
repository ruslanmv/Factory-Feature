{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1  Creation of the Dataframe of the project\n",
    "The project example is at the ..src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project='../src'\n",
    "path_project=\"../project_old\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add the utils directory to the system path\n",
    "sys.path.append(os.path.abspath('../'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "project_folder = \"project_old\"\n",
    "path_project = \"../\" + project_folder\n",
    "# Replace 'your_file.pkl' with the actual path to your pickle file\n",
    "file_path = f'./extraction/{project_folder}.pkl' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now you can import the function\n",
    "from utils.extractor import display_and_store_directory_content \n",
    "# Call the function with the desired path\n",
    "display_and_store_directory_content(path_project) \n",
    "clear_output()  # Clears the output after the current line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>readable</th>\n",
       "      <th>extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../project_old/utils</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../project_old/app.py</td>\n",
       "      <td>from utils.helpers import greet\\n\\ndef main():...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../project_old/requirements.txt</td>\n",
       "      <td># Python dependencies\\nflask</td>\n",
       "      <td>YES</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../project_old/utils/helpers.py</td>\n",
       "      <td>def greet(name):\\n    \"\"\"\\n    Returns a greet...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              path  \\\n",
       "0             ../project_old/utils   \n",
       "1            ../project_old/app.py   \n",
       "2  ../project_old/requirements.txt   \n",
       "3  ../project_old/utils/helpers.py   \n",
       "\n",
       "                                             content readable extension  \n",
       "0                                                         N/A       N/A  \n",
       "1  from utils.helpers import greet\\n\\ndef main():...      YES        py  \n",
       "2                       # Python dependencies\\nflask      YES       txt  \n",
       "3  def greet(name):\\n    \"\"\"\\n    Returns a greet...      YES        py  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Replace 'your_file.pkl' with the actual path to your pickle file\n",
    "# Now you can use file_path to read your pickle file\n",
    "# Load the DataFrame from the pickle file\n",
    "df = pd.read_pickle(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where 'readable' is not 'N/A'\n",
    "df_filtered = df[df['readable'] != 'N/A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['path', 'content', 'readable', 'extension'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the filtered DataFrame\n",
    "df_filtered.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>readable</th>\n",
       "      <th>extension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../project_old/app.py</td>\n",
       "      <td>from utils.helpers import greet\\n\\ndef main():...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../project_old/requirements.txt</td>\n",
       "      <td># Python dependencies\\nflask</td>\n",
       "      <td>YES</td>\n",
       "      <td>txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../project_old/utils/helpers.py</td>\n",
       "      <td>def greet(name):\\n    \"\"\"\\n    Returns a greet...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              path  \\\n",
       "1            ../project_old/app.py   \n",
       "2  ../project_old/requirements.txt   \n",
       "3  ../project_old/utils/helpers.py   \n",
       "\n",
       "                                             content readable extension  \n",
       "1  from utils.helpers import greet\\n\\ndef main():...      YES        py  \n",
       "2                       # Python dependencies\\nflask      YES       txt  \n",
       "3  def greet(name):\\n    \"\"\"\\n    Returns a greet...      YES        py  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM Watsonx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Man's best friend is his dog. Dogs are man's best friend because they are always there for you, they never judge you, and they always want to play. Dogs are also very smart, and they can learn tricks and commands. Dogs are also very good at detecting danger, and they can help keep you safe. Dogs are man's best friend because they are always there for you. \n"
     ]
    }
   ],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch credentials or prompt the user if missing\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if not WATSONX_APIKEY:\n",
    "    WATSONX_APIKEY = input(\"WML API key not found in .env. Please enter your WML API key: \").strip()\n",
    "    print(\"Reminder: Save your WML API key to the .env file for future use.\")\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = input(\"Project ID not found in .env. Please enter your project ID: \").strip()\n",
    "    print(\"Reminder: Save your Project ID to the .env file for future use.\")\n",
    "\n",
    "# Watsonx credentials\n",
    "credentials = {\n",
    "    \"url\": \"https://eu-gb.ml.cloud.ibm.com\",  # Update the URL as required\n",
    "    \"apikey\": WATSONX_APIKEY,\n",
    "    \"project_id\": PROJECT_ID,\n",
    "}\n",
    "\n",
    "# Example parameters for WatsonxLLM\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "# Initialize WatsonxLLM\n",
    "try:\n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        url=credentials[\"url\"],\n",
    "        project_id=credentials[\"project_id\"],\n",
    "        params=parameters,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error initializing WatsonxLLM: {e}\")\n",
    "\n",
    "# Function to invoke the WatsonxLLM model\n",
    "def invoke_model(prompt):\n",
    "    \"\"\"\n",
    "    Invokes the WatsonxLLM model with the given prompt.\n",
    "\n",
    "    :param prompt: The input prompt for the model.\n",
    "    :return: The response from the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = watsonx_llm.invoke(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during model invocation: {e}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample prompt for the model\n",
    "    prompt = \"Who is man's best friend?\"\n",
    "    try:\n",
    "        # Get the model's response\n",
    "        result = invoke_model(prompt)\n",
    "        print(f\"Response: {result}\")\n",
    "    except RuntimeError as e:\n",
    "        # Print any errors encountered\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 is the enhacement of the metadata of each element of the project.\n",
    "We create a column with a small description about the content using llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions generated and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch credentials or prompt the user if missing\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if not WATSONX_APIKEY:\n",
    "    WATSONX_APIKEY = input(\"WML API key not found in .env. Please enter your WML API key: \").strip()\n",
    "    print(\"Reminder: Save your WML API key to the .env file for future use.\")\n",
    "if not PROJECT_ID:\n",
    "    PROJECT_ID = input(\"Project ID not found in .env. Please enter your project ID: \").strip()\n",
    "    print(\"Reminder: Save your Project ID to the .env file for future use.\")\n",
    "\n",
    "# Watsonx credentials\n",
    "credentials = {\n",
    "    \"url\": \"https://eu-gb.ml.cloud.ibm.com\",  # Update the URL as required\n",
    "    \"apikey\": WATSONX_APIKEY,\n",
    "    \"project_id\": PROJECT_ID,\n",
    "}\n",
    "\n",
    "# Initialize WatsonxLLM\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"decoding_method\": \"greedy\",\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "try:\n",
    "    watsonx_llm = WatsonxLLM(\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        url=credentials[\"url\"],\n",
    "        project_id=credentials[\"project_id\"],\n",
    "        params=parameters,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error initializing WatsonxLLM: {e}\")\n",
    "\n",
    "# Function to invoke the WatsonxLLM model\n",
    "def invoke_model(prompt):\n",
    "    try:\n",
    "        response = watsonx_llm.invoke(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during model invocation: {e}\")\n",
    "\n",
    "# Function to generate a description for each text element\n",
    "def generate_description(df):\n",
    "    \"\"\"\n",
    "    Generates descriptions for each row in the DataFrame by parsing the 'text' column\n",
    "    and invoking WatsonxLLM for enhanced metadata creation.\n",
    "\n",
    "    :param df: The DataFrame containing the 'text' column.\n",
    "    :return: A new DataFrame with an added 'description' column.\n",
    "    \"\"\"\n",
    "    descriptions = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        prompt = f\"Provide a detailed summary description for the following content: {text}\"\n",
    "        try:\n",
    "            description = invoke_model(prompt)\n",
    "            descriptions.append(description)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error generating description for row: {e}\")\n",
    "            descriptions.append(\"Error generating description\")\n",
    "\n",
    "    # Add the descriptions to the DataFrame\n",
    "    df['description'] = descriptions\n",
    "    return df\n",
    "\n",
    "# Main logic\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the DataFrame\n",
    " \n",
    "    project_folder = \"project_old\"\n",
    "    #file_path = './extraction/src.pkl'\n",
    "    file_path = f'./extraction/{project_folder}.pkl' \n",
    "    df = pd.read_pickle(file_path)\n",
    "\n",
    "    # Create the 'text' column\n",
    "    def create_text_column(row):\n",
    "        text = f\"path: {row['path']} content: {row['content']} readable: {row['readable']} extension: {row['extension']}\"\n",
    "        return text\n",
    "\n",
    "    df['text'] = df.apply(create_text_column, axis=1)\n",
    "\n",
    "    # Generate descriptions and save results\n",
    "    try:\n",
    "        enhanced_df = generate_description(df)\n",
    "        enhanced_df.to_pickle(\"enhanced_dataframe.pkl\")\n",
    "        print(\"Descriptions generated and saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>readable</th>\n",
       "      <th>extension</th>\n",
       "      <th>text</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../project_old/utils</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>path: ../project_old/utils content:  readable:...</td>\n",
       "      <td>license: N/AThe path ../Project_old/utils is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../project_old/app.py</td>\n",
       "      <td>from utils.helpers import greet\\n\\ndef main():...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "      <td>path: ../project_old/app.py content: from util...</td>\n",
       "      <td>project: utils.helpers greet: def greet(name)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../project_old/requirements.txt</td>\n",
       "      <td># Python dependencies\\nflask</td>\n",
       "      <td>YES</td>\n",
       "      <td>txt</td>\n",
       "      <td>path: ../project_old/requirements.txt content:...</td>\n",
       "      <td>, py, zip, gzipped, tar.gz,bz2, egg, wheels\\nf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../project_old/utils/helpers.py</td>\n",
       "      <td>def greet(name):\\n    \"\"\"\\n    Returns a greet...</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "      <td>path: ../project_old/utils/helpers.py content:...</td>\n",
       "      <td>project.toml: utils: helpers: greet: def greet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              path  \\\n",
       "0             ../project_old/utils   \n",
       "1            ../project_old/app.py   \n",
       "2  ../project_old/requirements.txt   \n",
       "3  ../project_old/utils/helpers.py   \n",
       "\n",
       "                                             content readable extension  \\\n",
       "0                                                         N/A       N/A   \n",
       "1  from utils.helpers import greet\\n\\ndef main():...      YES        py   \n",
       "2                       # Python dependencies\\nflask      YES       txt   \n",
       "3  def greet(name):\\n    \"\"\"\\n    Returns a greet...      YES        py   \n",
       "\n",
       "                                                text  \\\n",
       "0  path: ../project_old/utils content:  readable:...   \n",
       "1  path: ../project_old/app.py content: from util...   \n",
       "2  path: ../project_old/requirements.txt content:...   \n",
       "3  path: ../project_old/utils/helpers.py content:...   \n",
       "\n",
       "                                         description  \n",
       "0   license: N/AThe path ../Project_old/utils is ...  \n",
       "1   project: utils.helpers greet: def greet(name)...  \n",
       "2  , py, zip, gzipped, tar.gz,bz2, egg, wheels\\nf...  \n",
       "3  project.toml: utils: helpers: greet: def greet...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Creation of the vector database from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Blog/Building-LLM-from-Scratch-in-Python/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added 4 documents to the collection.\n",
      "Collection 'my_vector_collection' now contains 4 documents.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Create the 'text' column\n",
    "def create_text_column(row):\n",
    "    text = f\"path: {row['path']} content: {row['content']} readable: {row['readable']} extension: {row['extension']}\"\n",
    "    return text\n",
    "\n",
    "# Define file path and collection name\n",
    "project_folder = \"project_old\"\n",
    "#file_path = './extraction/src.pkl'\n",
    "file_path = f'./extraction/{project_folder}.pkl' \n",
    "\n",
    "\n",
    "collection_name = \"my_vector_collection\"\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_pickle(file_path)\n",
    "df['text'] = df.apply(create_text_column, axis=1)\n",
    "df.insert(0, \"ID\", df.index.astype(str))  # Add unique ID for each row\n",
    "\n",
    "# Correct client initialization for ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Use PersistentClient for on-disk storage\n",
    "\n",
    "# Ensure the collection is deleted if it already exists\n",
    "if collection_name in [col.name for col in chroma_client.list_collections()]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "# Create a new collection\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "def create_embeddings_and_store(df, page_content_column, collection):\n",
    "    \"\"\"\n",
    "    This function generates embeddings from a dataframe and stores them in a Chroma collection.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The dataframe containing text data.\n",
    "        page_content_column (str): The name of the column containing the text content.\n",
    "        collection (chromadb.Collection): The Chroma collection to store the embeddings.\n",
    "    \"\"\"\n",
    "    # Configure embedding model\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_kwargs = {'device': device}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "    # Extract text content and generate embeddings\n",
    "    documents = df[page_content_column].tolist()\n",
    "    ids = df[\"ID\"].tolist()\n",
    "\n",
    "    # Generate embeddings for all documents\n",
    "    embeddings = hf.embed_documents(documents)  # Batch embedding generation\n",
    "\n",
    "    # Add documents, embeddings, and IDs to the collection\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=ids,\n",
    "        embeddings=embeddings,\n",
    "        metadatas=[{\"index\": i} for i in range(len(documents))]  # Example metadata\n",
    "    )\n",
    "\n",
    "    print(f\"Successfully added {len(documents)} documents to the collection.\")\n",
    "\n",
    "# Create embeddings and store them in Chroma\n",
    "create_embeddings_and_store(df, \"text\", collection)\n",
    "\n",
    "# Verify stored data\n",
    "print(f\"Collection '{collection_name}' now contains {collection.count()} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first 4 documents:\n",
      "Document ID: 0\n",
      "Content: path: ../project_old/utils content:  readable: N/A extension: N/A\n",
      "Metadata: {'index': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "Document ID: 1\n",
      "Content: path: ../project_old/app.py content: from utils.helpers import greet\n",
      "\n",
      "def main():\n",
      "    name = input(\"Enter your name: \")\n",
      "    print(greet(name))\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main() readable: YES extension: py\n",
      "Metadata: {'index': 1}\n",
      "--------------------------------------------------------------------------------\n",
      "Document ID: 2\n",
      "Content: path: ../project_old/requirements.txt content: # Python dependencies\n",
      "flask readable: YES extension: txt\n",
      "Metadata: {'index': 2}\n",
      "--------------------------------------------------------------------------------\n",
      "Document ID: 3\n",
      "Content: path: ../project_old/utils/helpers.py content: def greet(name):\n",
      "    \"\"\"\n",
      "    Returns a greeting message for the provided name.\n",
      "\n",
      "    Args:\n",
      "        name: The name of the person to greet.\n",
      "\n",
      "    Returns:\n",
      "        A greeting string.\n",
      "    \"\"\"\n",
      "    return f\"Hello, {name}! Welcome to the project.\" readable: YES extension: py\n",
      "Metadata: {'index': 3}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def display_first_five_documents(collection):\n",
    "    \"\"\"\n",
    "    Reads the first 5 documents from the given ChromaDB collection and displays them.\n",
    "\n",
    "    Args:\n",
    "        collection (chromadb.Collection): The ChromaDB collection to read from.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Retrieve all documents from the collection\n",
    "    results = collection.get()\n",
    "    \n",
    "    # Extract documents and IDs\n",
    "    documents = results.get(\"documents\", [])\n",
    "    ids = results.get(\"ids\", [])\n",
    "    metadatas = results.get(\"metadatas\", [])\n",
    "\n",
    "    # Display the first 5 documents\n",
    "    print(f\"Displaying the first {min(len(documents), 5)} documents:\")\n",
    "    for i in range(min(len(documents), 5)):\n",
    "        print(f\"Document ID: {ids[i]}\")\n",
    "        print(f\"Content: {documents[i]}\")\n",
    "        if metadatas:\n",
    "            print(f\"Metadata: {metadatas[i]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Usage Example\n",
    "display_first_five_documents(collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Inference of the LLM by using the vector Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- Generated response -----------------------------------\n",
      "{'question': 'How to integrate the specified features into the project components?', 'result': ' The project components are already integrated. The `app.py` file imports the `greet` function from `utils/helpers.py` and uses it in the `main` function. The `requirements.txt` file specifies the Python dependencies, including Flask. The `utils/helpers.py` file contains the `greet` function, which is a feature of the project. Therefore, the features are already integrated into the project components. No additional integration is needed. \\n\\nNote: The question is not asking about the functionality of the code, but rather how to integrate the features into the project components. The answer is that the features are already integrated.'}\n",
      "*********************************************************************************************\n",
      "{'question': 'How to integrate the specified features into the project components?', 'result': ' The project components are already integrated. The `app.py` file imports the `greet` function from `utils/helpers.py` and uses it in the `main` function. The `requirements.txt` file specifies the Python dependencies, including Flask. The `utils/helpers.py` file contains the `greet` function, which is a feature of the project. Therefore, the features are already integrated into the project components. No additional integration is needed. \\n\\nNote: The question is not asking about the functionality of the code, but rather how to integrate the features into the project components. The answer is that the features are already integrated.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from chromadb import PersistentClient\n",
    "import torch\n",
    "\n",
    "# Load credentials from .env file\n",
    "load_dotenv()\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if not WATSONX_APIKEY or not PROJECT_ID:\n",
    "    raise ValueError(\"API key or Project ID is missing. Please check your .env file.\")\n",
    "\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding_method, temperature):\n",
    "    \"\"\"\n",
    "    Initializes and returns a WatsonxLLM instance with the specified parameters.\n",
    "    \"\"\"\n",
    "    return WatsonxLLM(\n",
    "        model_id=model_type,\n",
    "        url=\"https://eu-gb.ml.cloud.ibm.com\",\n",
    "        project_id=PROJECT_ID,\n",
    "        params={\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"min_new_tokens\": min_tokens,\n",
    "            \"decoding_method\": decoding_method,\n",
    "            \"temperature\": temperature,\n",
    "        },\n",
    "    )\n",
    "\n",
    "def answer_questions_from_dataframe(question, collection_name, persist_directory=\"./chroma_db\"):\n",
    "    \"\"\"\n",
    "    Answers a question using a LangChain model and retrieves relevant documents from a Chroma collection.\n",
    "    \"\"\"\n",
    "    # Specify model parameters\n",
    "    model_type = \"meta-llama/llama-3-1-70b-instruct\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 100\n",
    "    decoding_method = \"greedy\"\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Initialize the WatsonxLLM model\n",
    "    model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding_method, temperature)\n",
    "\n",
    "    # Use the same embedding model as in create_embeddings_and_store for consistency\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_kwargs = {'device': device}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "    # Initialize Chroma Persistent Client\n",
    "    chroma_client = PersistentClient(path=persist_directory)\n",
    "\n",
    "    # Ensure the collection exists\n",
    "    if collection_name not in [col.name for col in chroma_client.list_collections()]:\n",
    "        raise ValueError(f\"Collection '{collection_name}' does not exist. Make sure it is created.\")\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "\n",
    "    # Use Chroma vectorstore with the given collection\n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=hf,\n",
    "        client=chroma_client,\n",
    "    )\n",
    "\n",
    "    # Create a retriever from the vectorstore\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # Build the RetrievalQA chain\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        input_key=\"question\"\n",
    "    )\n",
    "\n",
    "    # Run the chain with the question\n",
    "    response_text = chain.invoke({\"question\": question})\n",
    "\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text)\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# Example Usage\n",
    "question = \"How to integrate the specified features into the project components?\"\n",
    "\n",
    "# Ensure consistent settings\n",
    "persist_directory = \"./chroma_db\"\n",
    "collection_name = \"my_vector_collection\"\n",
    "\n",
    "response = answer_questions_from_dataframe(question, collection_name, persist_directory)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
