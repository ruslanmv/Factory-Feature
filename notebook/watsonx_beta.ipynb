{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DataFrameLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "try:\n",
    "    API_KEY = os.environ.get(\"API_KEY\")\n",
    "    project_id = os.environ.get(\"PROJECT_ID\")\n",
    "except KeyError:\n",
    "    API_KEY = input(\"Please enter your WML api key (hit enter): \")\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "\n",
    "    base_model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "    langchain_model = WatsonxLLM(model=base_model)\n",
    "\n",
    "    return langchain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions_from_dataframe(question, collection):\n",
    "  \"\"\"\n",
    "  This function answers a question using a Langchain model and retrieves relevant documents from a Chroma collection.\n",
    "\n",
    "  Args:\n",
    "      question (str): The question to be answered.\n",
    "      collection (chromadb.Collection): The Chroma collection containing document embeddings.\n",
    "\n",
    "  Returns:\n",
    "      str: The generated answer text.\n",
    "  \"\"\"\n",
    "\n",
    "  # Specify model parameters\n",
    "  model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "  max_tokens = 300\n",
    "  min_tokens = 100\n",
    "  decoding = DecodingMethods.GREEDY\n",
    "  temperature = 0.7\n",
    "\n",
    "  # Get the watsonx model that can be used with LangChain\n",
    "  model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "\n",
    "  # Use the same embedding model as in create_embeddings_and_store for consistency\n",
    "  from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "  import numpy as np  # Optional, for numerical operations on embeddings (if needed)\n",
    "\n",
    "  # Model configuration (should match the one used for creating embeddings)\n",
    "  model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "  model_kwargs = {'device': 'cpu'}  # Change to 'cuda' for GPU usage (if available)\n",
    "  encode_kwargs = {'normalize_embeddings': True}\n",
    "  # Initialize the embedding model:\n",
    "  hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "  embedding_question = hf.embed_query(question)  # Use embed_query for single text\n",
    "\n",
    "  def chroma_retriever(query_embedding):\n",
    "    # Find similar documents based on the query embedding\n",
    "    similar_ids = collection.nearest_neighbors(query_embedding, k=5)  # Retrieve top 5 similar documents\n",
    "    # Extract document texts based on the retrieved IDs\n",
    "    retrieved_documents = [collection.get(id_)[0][\"document\"] for id_ in similar_ids]\n",
    "    return retrieved_documents\n",
    "\n",
    "  # Wrap the chroma_retriever function in a dictionary (fixed)\n",
    "  retriever = {\"function\": chroma_retriever}\n",
    "\n",
    "  chain = RetrievalQA.from_chain_type(\n",
    "      llm=model,\n",
    "      chain_type=\"stuff\",\n",
    "      retriever=retriever,\n",
    "      input_key=\"question\"\n",
    "  )\n",
    "\n",
    "  response_text = chain.run(question)\n",
    "\n",
    "  print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "  print(response_text)\n",
    "  print(\"*********************************************************************************************\")\n",
    "\n",
    "  return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/output.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/output.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 1. Load the dataframe\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m))\n\u001b[0;32m     14\u001b[0m chroma_client \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mClient()\n",
      "File \u001b[1;32mc:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\pandas\\io\\pickle.py:190\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    197\u001b[0m \n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/output.pkl'"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "\n",
    "# Test answering questions based on the provided dataframe\n",
    "question = \"What are the limitations of generative AI models?\"\n",
    "\n",
    "# Provide the path relative to the dir in which the script is running\n",
    "file_path = \"../data/output.pkl\"\n",
    "\n",
    "# 1. Load the dataframe\n",
    "df = pd.read_pickle(file_path)\n",
    "df.insert(0, \"ID\", df.index.astype(str))\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection_name = \"my_vector_collection\"\n",
    "\n",
    "try:\n",
    "    collection = chroma_client.create_collection(name=collection_name)\n",
    "except:\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "def create_embeddings_and_store(df, page_content_column, collection):\n",
    "    \"\"\"\n",
    "    This function generates embeddings from a dataframe and stores them in a Chroma collection.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The dataframe containing text data.\n",
    "        page_content_column (str): The name of the column containing the text content.\n",
    "        collection (chromadb.Collection): The Chroma collection to store the embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    import numpy as np  # Optional, for numerical operations on embeddings (if needed)\n",
    "\n",
    "    # Choose an appropriate model:\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Replace with your desired model if needed\n",
    "    # Set device (CPU or GPU) based on your hardware and performance requirements:\n",
    "    model_kwargs = {'device': 'cpu'}  # Change to 'cuda' for GPU usage (if available)\n",
    "    # Encoding options (normalization is often recommended):\n",
    "    encode_kwargs = {'normalize_embeddings': True}  # Experiment with normalization\n",
    "\n",
    "    # Initialize the embedding model:\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "    # Extract the text content directly for Chroma's preferred format\n",
    "    documents = df[page_content_column].tolist()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[page_content_column]\n",
    "\n",
    "        # Corrected embedding generation:\n",
    "        embedding = hf.embed_query(text)  # Use embed_query for single text\n",
    "\n",
    "        # Corrected embedding extraction (no tolist() needed):\n",
    "        embedding_values = embedding  # The embedding is already a list of floats\n",
    "\n",
    "        # Print each document, embedding pair for debugging or verification (optional)\n",
    "        print(f\"Document: {text}\\nEmbedding: {embedding_values}\")\n",
    "\n",
    "    # Insert data into Chroma collection using preferred format\n",
    "    collection.add(documents=documents, ids=df['ID'].tolist())\n",
    "# 3. Create embeddings and store them in Chroma (if embeddings don't exist)\n",
    "# Comment out this step if the embeddings are already created and stored\n",
    "create_embeddings_and_store(df, \"Content\", collection)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question='What are the limitations of generative AI models?'\n",
    "# Specify model parameters\n",
    "model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "max_tokens = 300\n",
    "min_tokens = 100\n",
    "decoding = DecodingMethods.GREEDY\n",
    "temperature = 0.7\n",
    "# Get the watsonx model that can be used with LangChain\n",
    "model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "# Use the same embedding model as in create_embeddings_and_store for consistency\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "# Model configuration (should match the one used for creating embeddings)\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}  # Change to 'cuda' for GPU usage (if available)\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "# Initialize the embedding model:\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "embedding_question = hf.embed_query(question)  # Use embed_query for single text\n",
    "#  Query the collection\n",
    "context = collection.query(\n",
    "            query_texts=question,\n",
    "            n_results=2)\n",
    "prompt_template = '''Use the following pieces of context to answer the question at the end. \n",
    "If you do not know the answer, please think rationally and answer from your own knowledge base.  \n",
    "{context}\n",
    "Question: {question}\n",
    "'''\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "PROMPT = PromptTemplate(\n",
    "template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}  \n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever= , \n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "response_text = chain.run(question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
