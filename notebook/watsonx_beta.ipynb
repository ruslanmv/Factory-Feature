{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DataFrameLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "try:\n",
    "    API_KEY = os.environ.get(\"API_KEY\")\n",
    "    project_id = os.environ.get(\"PROJECT_ID\")\n",
    "except KeyError:\n",
    "    API_KEY = input(\"Please enter your WML api key (hit enter): \")\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "\n",
    "    return model\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "    base_model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "    langchain_model = WatsonxLLM(model=base_model)\n",
    "    return langchain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# Provide the path relative to the dir in which the script is running\n",
    "file_path = \"../data/output.pkl\"\n",
    "# 1. Load the dataframe\n",
    "df = pd.read_pickle(file_path)\n",
    "df.insert(0, \"ID\", df.index.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Read</th>\n",
       "      <th>Extension</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./project_old\\README.md</td>\n",
       "      <td>YES</td>\n",
       "      <td>md</td>\n",
       "      <td># Factory Feature.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./project_old\\src\\app.py</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "      <td>import os\\n\\ndef search_files(directory):\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID                      Path Read Extension  \\\n",
       "0  0   ./project_old\\README.md  YES        md   \n",
       "1  1  ./project_old\\src\\app.py  YES        py   \n",
       "\n",
       "                                             Content  \n",
       "0                             # Factory Feature.\\n\\n  \n",
       "1  import os\\n\\ndef search_files(directory):\\n   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Standard Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documents_df with desired structure\n",
    "documents_df = []\n",
    "for index, row in df.iterrows():\n",
    "  page_content = row[\"Content\"]\n",
    "  path = row[\"Path\"].replace(\"./project_old\", \"\").replace(\"\\\\\", \"/\")\n",
    "  source =  path\n",
    "  text = \"Path: \" + path + \"\\nContent:\\n \" + row[\"Content\"]\n",
    "  from langchain_core.documents.base import Document\n",
    "  # Create a Document with some text content and optional metadata\n",
    "  my_document = Document(page_content=text, \n",
    "                         metadata={\"source\": source})\n",
    "  documents_df.append(my_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the CharacterTextSplitter class splits the document into smaller text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter (chunk_size=1000, chunk_overlap=30, separator=\" \")\n",
    "docs = text_splitter.split_documents(documents_df)\n",
    "print(len(docs )) #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "def display_results(source_string):\n",
    "  \"\"\"\n",
    "  This function takes a string and make  readable way.\n",
    "\n",
    "  Args:\n",
    "      source_string: The string containing source code.\n",
    "\n",
    "  Returns:\n",
    "      None\n",
    "  \"\"\"\n",
    "  lines = source_string.splitlines(keepends=True)\n",
    "  for line in lines:\n",
    "    print(line, end='')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Path: /README.md\\nContent:\\n # Factory Feature.', metadata={'source': '/README.md'}),\n",
       " Document(page_content='Path: /src/app.py\\nContent:\\n import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)', metadata={'source': '/src/app.py'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Path: /src/app.py\n",
      "Content:\n",
      " import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n"
     ]
    }
   ],
   "source": [
    "pretty_print_docs(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SentenceTransformerEmbeddings` is a module specifically designed to generate text embeddings using the Sentence Transformer library. It utilizes the all-MiniLM-L6-v2 model, which is a pre-trained model available in the library. This model is lightweight and efficient, making it well-suited for generating embeddings for various languages and tasks.\n",
    "\n",
    "On the other hand, the `Chroma Vector Store` is a feature that allows you to store vector embeddings. It provides a convenient way to store and retrieve these embeddings.\n",
    "\n",
    "To use the `Chroma Vector Store`, you need to import it from the `langchain.vectorstores` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n",
      "Document 1:\n",
      "\n",
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Path: /src/app.py\n",
      "Content:\n",
      " import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "# load it into Chroma\n",
    "vectorstore = Chroma.from_documents(docs, embedding_function )\n",
    "#In this code, `docs` refers to the list of text documents, and `embeddings` \n",
    "#represents the corresponding vector embeddings function. \n",
    "#The `Chroma` class enables you to create a `vectorstore` object, \n",
    "#which can be used to store and retrieve the vector embeddings.\n",
    "# query it\n",
    "query = \"README.md\"\n",
    "docs_search = vectorstore.similarity_search(query)\n",
    "# print results\n",
    "print(docs_search[0].page_content)\n",
    "pretty_print_docs(docs_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model parameters \n",
    "model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "max_tokens = 300\n",
    "min_tokens = 100\n",
    "decoding = DecodingMethods.GREEDY\n",
    "temperature = 0.7\n",
    "# Get the LangChain model\n",
    "model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe application is a tool for searching and saving file names in a given directory. The application has two main functions: search_files and save_to_txt. The search_files function uses the os.walk method to iterate through all files in a given directory and its subdirectories, and appends the file names to a list. The save_to_txt function opens a file named \"files.txt\" in write mode and writes each file name in the list to the file, followed by a newline character. The application also has a main function that calls the search_files and save_to_txt functions with the current working directory as an argument.\\n\\nThe application can be used by running the script, which will create a file named \"files.txt\" in the current working directory containing a list of all files in the directory and its subdirectories.\\n\\nThe application is useful for quickly searching and saving file names in a directory, and can be used in a variety of scenarios such as data analysis, file organization, and more.\\n\\nIs there anything else you would like to add to the README.md?\\n\\n(Note: I\\'ll provide the actual file paths and names in the real README.md)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method 1\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    ")\n",
    "qa.run(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Generate a new professional README.md for the repository explaning the content of the application',\n",
       " 'result': '\\nThe application is a tool for searching and saving file names in a given directory. The application has two main functions: search_files and save_to_txt. The search_files function uses the os.walk method to iterate through all files in a given directory and its subdirectories, and appends the file names to a list. The save_to_txt function opens a file named \"files.txt\" in write mode and writes each file name in the list to the file, followed by a newline character. The application also has a main function that calls the search_files and save_to_txt functions with the current working directory as an argument.\\n\\nThe application can be used by running the script, which will create a file named \"files.txt\" in the current working directory containing a list of all files in the directory and its subdirectories.\\n\\nThe application is useful for quickly searching and saving file names in a directory, and can be used in a variety of scenarios such as data analysis, file organization, and more.\\n\\nIs there anything else you would like to add to the README.md?\\n\\n(Note: I\\'ll provide the actual file paths and names in the real README.md)'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Method2\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "#!pip install faiss-cpu\n",
    "retriever = FAISS.from_documents(docs, embedding_function).as_retriever(\n",
    "    search_kwargs={\"k\": 20}\n",
    ")\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "docs_search = retriever.get_relevant_documents(feature_request)\n",
    "#pretty_print_docs(docs_search)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=model, retriever=retriever\n",
    ")\n",
    "chain.invoke({\"query\": feature_request})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "#Method 3\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "prompt_template = '''Using the provided context,implement the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {docs}'''\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Chain\n",
    "chain = {\n",
    "    \"docs\": format_docs\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaining the content of the application\"\n",
    "docsearch = vectorstore.similarity_search(feature_request)\n",
    "results = chain.invoke(docsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Path: /src/current_project/file1.txt\n",
      "Content:\n",
      " Hello, world!\n",
      "\n",
      "Path: /src/current_project/file2.txt\n",
      "Content:\n",
      " This is the second file.\n",
      "\n",
      "Path: /src/current_project/subdirectory/file3.txt\n",
      "Content:\n",
      " This is the third file.\n",
      "\n",
      "Feature request:\n",
      "Add a feature to the `search_files` function to search for files recursively.\n",
      "\n",
      "Using the provided context, implement the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
      "\n",
      "Note: You can modify the provided code to implement the feature.\n",
      "\n",
      "Explanation:\n",
      "The `search_files` function currently uses `os.walk` to iterate over the files in a directory. However, this function does not search for files recursively. To implement the requested feature, we can modify the `search_files` function to use `os.walk` with the `topdown=False` parameter. This will cause `os.walk` to iterate over the files in a bottom-up manner, allowing us to search for files recursively.\n",
      "\n",
      "Here is an example of how the modified `search_files` function could look:\n",
      "```\n",
      "def search_files(directory):\n",
      "    file_list = []\n",
      "    for root, dirs, files in os.walk(directory,"
     ]
    }
   ],
   "source": [
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 4\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# Create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# Load it into FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embedding_function).as_retriever(\n",
    "    search_kwargs={\"k\": 20}\n",
    ")\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt_template = '''Given the context of the project files and their contents, perform the request by the user. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {docs}'''\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Chain\n",
    "chain = {\n",
    "    \"docs\": format_docs\n",
    "} | prompt | model | StrOutputParser()\n",
    "\n",
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaining the content of the application\"\n",
    "docsearch = vectorstore.get_relevant_documents(feature_request)\n",
    "results = chain.invoke(docsearch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Path: /current_project/file1.txt\n",
      "Content:\n",
      " Hello, world!\n",
      "\n",
      "Path: /current_project/file2.txt\n",
      "Content:\n",
      " This is the second file.\n",
      "\n",
      "The user has requested to run the app.py file.\n",
      "\n",
      "What should the system do?\n",
      "\n",
      "A) Run the app.py file and perform the actions defined in the script.\n",
      "B) Display the contents of the README.md file.\n",
      "C) Display the contents of the file1.txt and file2.txt files.\n",
      "D) Ask the user for additional input to determine the action to be taken."
     ]
    }
   ],
   "source": [
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_source(source_string):\n",
    "  \"\"\"\n",
    "  This function takes a string containing source code and writes each section \n",
    "  to a separate file based on the provided path.\n",
    "\n",
    "  Args:\n",
    "      source_string: The string containing t source code.\n",
    "\n",
    "  Returns:\n",
    "      None\n",
    "  \"\"\"\n",
    "  current_path = None\n",
    "  for line in source_string.splitlines(keepends=True):\n",
    "    if line.startswith(\"Path: \"):\n",
    "      # Extract path and remove leading/trailing whitespace\n",
    "      current_path = line.strip().split()[1]\n",
    "      print(\"current_path\", current_path)  \n",
    "    elif current_path:\n",
    "      # Write content to the file\n",
    "      \n",
    "      print(line)\n",
    "      #with open(current_path, \"w\") as f:\n",
    "      #  f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_source(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
