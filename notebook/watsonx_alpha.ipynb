{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DataFrameLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "try:\n",
    "    API_KEY = os.environ.get(\"API_KEY\")\n",
    "    project_id = os.environ.get(\"PROJECT_ID\")\n",
    "except KeyError:\n",
    "    API_KEY = input(\"Please enter your WML api key (hit enter): \")\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "\n",
    "    return model\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "    base_model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "    langchain_model = WatsonxLLM(model=base_model)\n",
    "    return langchain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "# Provide the path relative to the dir in which the script is running\n",
    "file_path = \"../data/output.pkl\"\n",
    "# 1. Load the dataframe\n",
    "df = pd.read_pickle(file_path)\n",
    "df.insert(0, \"ID\", df.index.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Read</th>\n",
       "      <th>Extension</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./project_old\\README.md</td>\n",
       "      <td>YES</td>\n",
       "      <td>md</td>\n",
       "      <td># Factory Feature.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./project_old\\src\\app.py</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "      <td>import os\\n\\ndef search_files(directory):\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID                      Path Read Extension  \\\n",
       "0  0   ./project_old\\README.md  YES        md   \n",
       "1  1  ./project_old\\src\\app.py  YES        py   \n",
       "\n",
       "                                             Content  \n",
       "0                             # Factory Feature.\\n\\n  \n",
       "1  import os\\n\\ndef search_files(directory):\\n   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextLoader class loads the content of some text into a list named documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('text.txt')\n",
    "documents_txt = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Text Information', metadata={'source': 'some_text.txt'})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents_txt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Standard Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documents_df with desired structure\n",
    "documents_df = []\n",
    "for index, row in df.iterrows():\n",
    "  page_content = row[\"Content\"]\n",
    "  path = row[\"Path\"].replace(\"./project_old\", \"\").replace(\"\\\\\", \"/\")\n",
    "  source =  path\n",
    "  from langchain_core.documents.base import Document\n",
    "  # Create a Document with some text content and optional metadata\n",
    "  my_document = Document(page_content=page_content, \n",
    "                         metadata={\"source\": source})\n",
    "  documents_df.append(my_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the CharacterTextSplitter class splits the document into smaller text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter (chunk_size=1000, chunk_overlap=30, separator=\" \")\n",
    "docs = text_splitter.split_documents(documents_df)\n",
    "print(len(docs )) #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Factory Feature.', metadata={'source': '/README.md'}),\n",
       " Document(page_content='import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)', metadata={'source': '/src/app.py'})]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SentenceTransformerEmbeddings` is a module specifically designed to generate text embeddings using the Sentence Transformer library. It utilizes the all-MiniLM-L6-v2 model, which is a pre-trained model available in the library. This model is lightweight and efficient, making it well-suited for generating embeddings for various languages and tasks.\n",
    "\n",
    "On the other hand, the `Chroma Vector Store` is a feature that allows you to store vector embeddings. It provides a convenient way to store and retrieve these embeddings.\n",
    "\n",
    "To use the `Chroma Vector Store`, you need to import it from the `langchain.vectorstores` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer  # Import the correct class\n",
    "# Extract text content from texts\n",
    "texts_content = [doc.page_content for doc in docs]  # List comprehension\n",
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Pass the model name directly\n",
    "# Generate embeddings for each text\n",
    "embeddings_sample = model.encode(texts_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding: [-0.04426908 -0.0081234   0.04465149 -0.0182588   0.03830957  0.04288102\n",
      "  0.0580455  -0.00669592 -0.12391116 -0.05695332]\n"
     ]
    }
   ],
   "source": [
    "# Print the generated embedding\n",
    "print(\"Text Embedding:\", embeddings_sample[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, `docs` refers to the list of text documents, and `embeddings` represents the corresponding vector embeddings function. The `Chroma` class enables you to create a `db` object, which can be used to store and retrieve the vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1eb9c97b190>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = \"README.md\"\n",
    "docs_search = db.similarity_search(query)\n",
    "# print results\n",
    "print(docs_search[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the previous example, if you want to save to disk, simply initialize the Chroma client and pass the directory where you want the data to be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n"
     ]
    }
   ],
   "source": [
    "# save to disk\n",
    "db = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "# load from disk\n",
    "db2 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "docs = db2.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: Chroma makes a best-effort to automatically save data to disk, however multiple in-memory clients can stop each otherâ€™s work. As a best practice, only have one client per path running at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **From Texts:**\n",
    "\n",
    "   - If you have plain text documents, convert them into a list called `texts`.\n",
    "   - Use `Chroma.from_texts()` to create `db`, specifying `texts` along with an optional `embedding_function`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Querying Chroma**\n",
    "\n",
    "Chroma offers several ways to search for documents based on textual similarity:\n",
    "\n",
    "1. **Basic Textual Search (Similarity Search):**\n",
    "\n",
    "   - Provide a query string to `db.similarity_search()`.\n",
    "   - This method returns the `k` most similar documents (default `k=4`) based on cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '/src/app.py'}\n",
      "import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n",
      "{'source': '/src/app.py'}\n",
      "import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n",
      "{'source': '/src/app.py'}\n",
      "import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n",
      "{'source': '/src/app.py'}\n",
      "import os\n",
      "\n",
      "def search_files(directory):\n",
      " file_list = []\n",
      " for root, dirs, files in os.walk(directory):\n",
      " for file in files:\n",
      " file_list.append(os.path.join(root, file))\n",
      " return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      " with open(\"files.txt\", \"w\") as file:\n",
      " for file_name in file_list:\n",
      " file.write(file_name + \"\\n\")\n",
      " print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      " directory = \"./current_project\"\n",
      " file_list = search_files(directory)\n",
      " save_to_txt(file_list)\n"
     ]
    }
   ],
   "source": [
    "query = \"README.md\"\n",
    "docs_search = db.similarity_search(query)\n",
    "# Access document content:\n",
    "for doc in docs_search:\n",
    "    print(doc.metadata)  \n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Search with Filtering:**\n",
    "\n",
    "   - Use the `filter` argument in `similarity_search()` to restrict results based on metadata associated with documents. The `filter` argument is a dictionary with metadata field names as keys and desired values as values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"README.md\"\n",
    "filter_criteria = {\"id\": \"0\"}  # Search for documents with ID \"doc2\"\n",
    "filtered_docs = db.similarity_search(query, filter=filter_criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in filtered_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Search with Score Threshold:**\n",
    "\n",
    "   - The `similarity_search_with_relevance_scores()` method retrieves documents along with their similarity scores (cosine distances) between 0 (most similar) and 1 (least similar).\n",
    "   - Optionally, you can use the `score_threshold` keyword argument to filter results based on a minimum similarity score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\langchain_core\\vectorstores.py:311: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)', metadata={'source': '/src/app.py'}), -0.2994511969663003), (Document(page_content='import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)', metadata={'source': '/src/app.py'}), -0.2994511969663003), (Document(page_content='import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)', metadata={'source': '/src/app.py'}), -0.2994511969663003), (Document(page_content='import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)', metadata={'source': '/src/app.py'}), -0.2994511969663003)]\n",
      "  warnings.warn(\n",
      "c:\\Users\\066226758\\workspace\\workshop\\.venv\\lib\\site-packages\\langchain_core\\vectorstores.py:323: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.7\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "docs_with_scores = db.similarity_search_with_relevance_scores(query, score_threshold=0.7)\n",
    "for doc, score in docs_with_scores:\n",
    "    print(doc, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Maximal Marginal Relevance (MMR) Search:**\n",
    "\n",
    "   - The `max_marginal_relevance_search()` method aims to return documents that are both relevant to the query and diverse from each other. Use this when you want a set of documents that covers a broader range of related topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 8, updating n_results = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='import os\\n\\ndef search_files(directory):\\n file_list = []\\n for root, dirs, files in os.walk(directory):\\n for file in files:\\n file_list.append(os.path.join(root, file))\\n return file_list\\n\\ndef save_to_txt(file_list):\\n with open(\"files.txt\", \"w\") as file:\\n for file_name in file_list:\\n file.write(file_name + \"\\\\n\")\\n print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n directory = \"./current_project\"\\n file_list = search_files(directory)\\n save_to_txt(file_list)' metadata={'source': '/src/app.py'}\n",
      "page_content='# Factory Feature.' metadata={'source': '/README.md'}\n"
     ]
    }
   ],
   "source": [
    "mmr_results = db.max_marginal_relevance_search(query, k=2, lambda_mult=0.15)  # More diversity\n",
    "for doc in mmr_results:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2 - Collection in Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Path: /README.md\\nContent:\\n # Factory Feature.\\n\\n', 'Path: /src/app.py\\nContent:\\n import os\\n\\ndef search_files(directory):\\n    file_list = []\\n    for root, dirs, files in os.walk(directory):\\n        for file in files:\\n            file_list.append(os.path.join(root, file))\\n    return file_list\\n\\ndef save_to_txt(file_list):\\n    with open(\"files.txt\", \"w\") as file:\\n        for file_name in file_list:\\n            file.write(file_name + \"\\\\n\")\\n    print(\"File names saved to files.txt\")\\n\\nif __name__ == \"__main__\":\\n    directory = \"./current_project\"\\n    file_list = search_files(directory)\\n    save_to_txt(file_list)\\n    \\n']\n"
     ]
    }
   ],
   "source": [
    "## Generation of the document Text\n",
    "# Extract the text content directly for Chroma's preferred format\n",
    "documents = []\n",
    "paths=[]\n",
    "for index, row in df.iterrows():\n",
    "    path = row[\"Path\"].replace(\"./project_old\", \"\").replace(\"\\\\\", \"/\")\n",
    "    text = \"Path: \" + path + \"\\nContent:\\n \" + row[\"Content\"]\n",
    "    paths.append(path[1:])\n",
    "    #print(text)\n",
    "    documents.append(text)\n",
    "print(documents)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md', 'src/app.py']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=[str(i) for i in range(len(documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection = persistent_client.get_or_create_collection(\"collection_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n"
     ]
    }
   ],
   "source": [
    "collection.add(ids=ids, documents=documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 in the collection\n"
     ]
    }
   ],
   "source": [
    "langchain_chroma = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embedding_function,\n",
    ")\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1eb9d08e6b0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n",
      "\n",
      "\n",
      "{}\n",
      "Path: /src/app.py\n",
      "Content:\n",
      " import os\n",
      "\n",
      "def search_files(directory):\n",
      "    file_list = []\n",
      "    for root, dirs, files in os.walk(directory):\n",
      "        for file in files:\n",
      "            file_list.append(os.path.join(root, file))\n",
      "    return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      "    with open(\"files.txt\", \"w\") as file:\n",
      "        for file_name in file_list:\n",
      "            file.write(file_name + \"\\n\")\n",
      "    print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    directory = \"./current_project\"\n",
      "    file_list = search_files(directory)\n",
      "    save_to_txt(file_list)\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"README.md\"\n",
    "docs_search = langchain_chroma.similarity_search(query)\n",
    "# Access document content:\n",
    "for doc in docs_search:\n",
    "    print(doc.metadata)  \n",
    "    print(doc.page_content)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    }
   ],
   "source": [
    "query = \"README.md\"\n",
    "filter_criteria = {\"id\": \"0\"}  # Search for documents with ID \"0\"\n",
    "docs_search = langchain_chroma.similarity_search(query,filter=filter_criteria)\n",
    "# Access document content:\n",
    "for doc in docs_search:\n",
    "    print(doc.metadata)  \n",
    "    print(doc.page_content)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We create a Colletion: collection_name\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collectionname=\"collection_name\"\n",
    "try:\n",
    "    print(\"We create a Colletion:\",collectionname)\n",
    "    collection1 = chroma_client.create_collection(\n",
    "        name=collectionname)  \n",
    "except:\n",
    "    print(\"We load a Colletion:\",collectionname)\n",
    "    collection1 = chroma_client.get_collection(name=collectionname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection1.add(ids=ids, documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embeddings(text):\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    import numpy as np  # Optional\n",
    "    # Choose an appropriate model:\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Replace with your desired model if needed\n",
    "    # Set device (CPU or GPU) based on your hardware and performance requirements:\n",
    "    model_kwargs = {'device': 'cpu'}  # Change to 'cuda' for GPU usage (if available)\n",
    "    # Encoding options (normalization is often recommended):\n",
    "    encode_kwargs = {'normalize_embeddings': True}  # Experiment with normalization\n",
    "    # Initialize the embedding model:\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)    \n",
    "    embedding = hf.embed_query(text)  # Use embed_query for single text\n",
    "    return embedding\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "embedding_feature = create_embeddings(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model parameters \n",
    "model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "max_tokens = 300\n",
    "min_tokens = 100\n",
    "decoding = DecodingMethods.GREEDY\n",
    "temperature = 0.7\n",
    "# Get the LangChain model\n",
    "model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "context = collection.query(query_texts=feature_request, n_results=2)\n",
    "prompt_template = '''\n",
    "Using the provided context of the project pieces, please generate a new code implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {context}\n",
    "Feature Request: {feature_request}\n",
    "'''\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"feature_request\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\", \n",
    "    retriever= , \n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")\n",
    "response_text = chain.run(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': PromptTemplate(input_variables=['context', 'feature_request'], template='\\nUsing the provided context of the project pieces, please generate a new code implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\\nContext: {context}\\nFeature Request: {feature_request}\\n')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_type_kwargs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
