{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reading credentials from the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, DataFrameLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# WML python SDK\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "# Load API credentials from .env file\n",
    "load_dotenv()\n",
    "try:\n",
    "    API_KEY = os.environ.get(\"API_KEY\")\n",
    "    project_id = os.environ.get(\"PROJECT_ID\")\n",
    "except KeyError:\n",
    "    API_KEY = input(\"Please enter your WML api key (hit enter): \")\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": API_KEY\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "\n",
    "    generate_params = {\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.TEMPERATURE: temperature\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_type,\n",
    "        params=generate_params,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "    )\n",
    "\n",
    "    return model\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature):\n",
    "    base_model = get_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "    langchain_model = WatsonxLLM(model=base_model)\n",
    "    return langchain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "# Provide the path relative to the dir in which the script is running\n",
    "file_path = \"../data/output.pkl\"\n",
    "# 1. Load the dataframe\n",
    "df = pd.read_pickle(file_path)\n",
    "df.insert(0, \"ID\", df.index.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Path</th>\n",
       "      <th>Read</th>\n",
       "      <th>Extension</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>./project_old\\README.md</td>\n",
       "      <td>YES</td>\n",
       "      <td>md</td>\n",
       "      <td># Factory Feature.\\n\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>./project_old\\src\\app.py</td>\n",
       "      <td>YES</td>\n",
       "      <td>py</td>\n",
       "      <td>import os\\n\\ndef search_files(directory):\\n   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID                      Path Read Extension  \\\n",
       "0  0   ./project_old\\README.md  YES        md   \n",
       "1  1  ./project_old\\src\\app.py  YES        py   \n",
       "\n",
       "                                             Content  \n",
       "0                             # Factory Feature.\\n\\n  \n",
       "1  import os\\n\\ndef search_files(directory):\\n   ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We create a Colletion: my_vector_collection\n",
      "We load a Colletion: my_vector_collection\n",
      "Path: /README.md\n",
      "Content:\n",
      " # Factory Feature.\n",
      "\n",
      "\n",
      "Path: /src/app.py\n",
      "Content:\n",
      " import os\n",
      "\n",
      "def search_files(directory):\n",
      "    file_list = []\n",
      "    for root, dirs, files in os.walk(directory):\n",
      "        for file in files:\n",
      "            file_list.append(os.path.join(root, file))\n",
      "    return file_list\n",
      "\n",
      "def save_to_txt(file_list):\n",
      "    with open(\"files.txt\", \"w\") as file:\n",
      "        for file_name in file_list:\n",
      "            file.write(file_name + \"\\n\")\n",
      "    print(\"File names saved to files.txt\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    directory = \"./current_project\"\n",
      "    file_list = search_files(directory)\n",
      "    save_to_txt(file_list)\n",
      "    \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection_name = \"my_vector_collection\"\n",
    "# Delete the existing collection \n",
    "#chroma_client.delete_collection(collection_name)\n",
    "try:\n",
    "    print(\"We create a Colletion:\",collection_name)\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name)  \n",
    "except:\n",
    "    print(\"We load a Colletion:\",collection_name)\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "def create_embeddings(text):\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    import numpy as np  # Optional\n",
    "    # Choose an appropriate model:\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"  # Replace with your desired model if needed\n",
    "    # Set device (CPU or GPU) based on your hardware and performance requirements:\n",
    "    model_kwargs = {'device': 'cpu'}  # Change to 'cuda' for GPU usage (if available)\n",
    "    # Encoding options (normalization is often recommended):\n",
    "    encode_kwargs = {'normalize_embeddings': True}  # Experiment with normalization\n",
    "    # Initialize the embedding model:\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)    \n",
    "    embedding = hf.embed_query(text)  # Use embed_query for single text\n",
    "    return embedding\n",
    "\n",
    "def create_embeddings_and_store(df, page_content_column, collection):\n",
    "    \"\"\"\n",
    "    This function generates embeddings from a dataframe and stores them in a Chroma collection.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The dataframe containing text data.\n",
    "        page_content_column (str): The name of the column containing the text content.\n",
    "        collection (chromadb.Collection): The Chroma collection to store the embeddings.\n",
    "    \"\"\"\n",
    "    # Extract the text content directly for Chroma's preferred format\n",
    "    documents = df[page_content_column].tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        path= row[\"Path\"].replace(\"./project_old\", \"\").replace(\"\\\\\", \"/\")\n",
    "        text = \"Path: \" + path + \"\\nContent:\\n \" + row[page_content_column]\n",
    "        print(text)\n",
    "        # Corrected embedding generation:\n",
    "        embedding = create_embeddings(text)  # Use embed_query for single text\n",
    "        # Corrected embedding extraction :\n",
    "        embedding_values = embedding  # The embedding is already a list of floats\n",
    "        # Print each document, embedding pair for debugging or verification (optional)\n",
    "        #print(f\"Document: {text}\\nEmbedding: {embedding_values}\")\n",
    "    # Insert data into Chroma collection using preferred format\n",
    "    collection.add(documents=documents, ids=df['ID'].tolist())\n",
    "# 3. Create embeddings and store them in Chroma (if embeddings don't exist)\n",
    "# Comment out this step if the embeddings are already created and stored\n",
    "create_embeddings_and_store(df, \"Content\", collection)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate the feature prompt that you want to include in the project\n",
    "feature_request = \"Generate a new professional README.md for the repository explaning the content of the application\"\n",
    "embedding_feature = create_embeddings(feature_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model parameters \n",
    "model_type = \"meta-llama/llama-2-70b-chat\"\n",
    "max_tokens = 300\n",
    "min_tokens = 100\n",
    "decoding = DecodingMethods.GREEDY\n",
    "temperature = 0.7\n",
    "# Get the LangChain model\n",
    "model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding, temperature)\n",
    "context = collection.query(query_texts=feature_request, n_results=2)\n",
    "prompt_template = '''\n",
    "Using the provided context of the project pieces, please generate a new code implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\n",
    "Context: {context}\n",
    "Feature Request: {feature_request}\n",
    "'''\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"feature_request\"])\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': PromptTemplate(input_variables=['context', 'feature_request'], template='\\nUsing the provided context of the project pieces, please generate a new code implementing the requested feature. If you do not know the answer, make a rational decision based on your knowledge.\\nContext: {context}\\nFeature Request: {feature_request}\\n')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_type_kwargs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
