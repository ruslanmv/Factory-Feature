{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from src.analysis.project_parser import parse_project\n",
    "from src.analysis.dependency_resolver import resolve_dependencies\n",
    "from src.analysis.feature_mapper import map_features_to_components\n",
    "from src.generation.project_generator import generate_project\n",
    "from src.generation.feature_integration import integrate_features\n",
    "\n",
    "from src.models.prompt_templates import get_prompt_template\n",
    "from src.utils.logger import logger\n",
    "from src.utils.file_operations import read_file, write_file\n",
    "from src.vector_database.db_builder import build_vector_database\n",
    "from src.vector_database.db_query import query_vector_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------- Generated response -----------------------------------\n",
      "{'query': 'How to integrate the specified features into the project components?', 'result': ' You can use the `integrate_features` function from the `feature_integration.py` module, which takes a list of project components and feature instructions as input and modifies the components accordingly. The function writes the feature instructions to the end of each component file. For example: `integrate_features([\"component1.py\", \"component2.py\"], \"feature_instructions\")`. \\n\\nNote: The `integrate_features` function does not return any value, it modifies the project components in-place. \\n\\nAlso, you can use the `map_features_to_components` function from the `feature_mapper.py` module to map feature requests to specific project components. This function takes parsed project data and a user-provided feature request as input and returns a mapping of features to components. \\n\\nFor example: `map_features_to_components(project_data, \"feature_request\")`. \\n\\nYou can then use the `integrate_features` function to integrate the features into the project components. \\n\\nFor example: `integrate_features(mapped_components, \"feature_instructions\")`. \\n\\nNote: The `integrate_features` function does not return any value, it modifies the project components in-place. \\n\\nYou can also use the `get_prompt_template` function from the `prompt_templates.py` module to get a template for feature integration prompts. \\n\\nFor example: `get_prompt_template()`. \\n\\nYou can then use this template to create a prompt for feature integration. \\n\\nFor example: `prompt = get_prompt_template().format(feature_request=\"feature'}\n",
      "*********************************************************************************************\n",
      "{'query': 'How to integrate the specified features into the project components?', 'result': ' You can use the `integrate_features` function from the `feature_integration.py` module, which takes a list of project components and feature instructions as input and modifies the components accordingly. The function writes the feature instructions to the end of each component file. For example: `integrate_features([\"component1.py\", \"component2.py\"], \"feature_instructions\")`. \\n\\nNote: The `integrate_features` function does not return any value, it modifies the project components in-place. \\n\\nAlso, you can use the `map_features_to_components` function from the `feature_mapper.py` module to map feature requests to specific project components. This function takes parsed project data and a user-provided feature request as input and returns a mapping of features to components. \\n\\nFor example: `map_features_to_components(project_data, \"feature_request\")`. \\n\\nYou can then use the `integrate_features` function to integrate the features into the project components. \\n\\nFor example: `integrate_features(mapped_components, \"feature_instructions\")`. \\n\\nNote: The `integrate_features` function does not return any value, it modifies the project components in-place. \\n\\nYou can also use the `get_prompt_template` function from the `prompt_templates.py` module to get a template for feature integration prompts. \\n\\nFor example: `get_prompt_template()`. \\n\\nYou can then use this template to create a prompt for feature integration. \\n\\nFor example: `prompt = get_prompt_template().format(feature_request=\"feature'}\n"
     ]
    }
   ],
   "source": [
    "from src.models.llm_inference import answer_questions_from_dataframe\n",
    "question = \"How to integrate the specified features into the project components?\"\n",
    "persist_directory = \"./notebooks/chroma_db\"\n",
    "collection_name = \"my_vector_collection\"\n",
    "response = answer_questions_from_dataframe(question, collection_name, persist_directory)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Received prompt: Who is man's best friend?\n",
      "Debug: Vector DB provided: False\n",
      "Debug: Formatted prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "None<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Who is man's best friend?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Blog/Building-LLM-from-Scratch-in-Python/.venv/lib/python3.10/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:415: LifecycleWarning: Model 'meta-llama/llama-3-70b-instruct' is in deprecated state from 2024-12-02 until 2025-02-03. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Querying without vector database...\n",
      "Debug: Raw response from model: That's an easy one! Man's best friend is the dog! Dogs have been human companions for thousands of years, providing loyalty, affection, and protection. They are often referred to as \"man's best friend\" because of their unique ability to form strong bonds with humans and provide comfort, companionship, and assistance.\n",
      "Debug: Response as string: That's an easy one! Man's best friend is the dog! Dogs have been human companions for thousands of years, providing loyalty, affection, and protection. They are often referred to as \"man's best friend\" because of their unique ability to form strong bonds with humans and provide comfort, companionship, and assistance.\n",
      "That's an easy one! Man's best friend is the dog! Dogs have been human companions for thousands of years, providing loyalty, affection, and protection. They are often referred to as \"man's best friend\" because of their unique ability to form strong bonds with humans and provide comfort, companionship, and assistance.\n"
     ]
    }
   ],
   "source": [
    "from src.models.llm_inference import query_llm\n",
    "prompt = \"Who is man's best friend?\"\n",
    "response = query_llm(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Received prompt: How to integrate the specified features into the project components?\n",
      "Debug: Vector DB provided: True\n",
      "Debug: Formatted prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "None<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "How to integrate the specified features into the project components?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Blog/Building-LLM-from-Scratch-in-Python/.venv/lib/python3.10/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:415: LifecycleWarning: Model 'meta-llama/llama-3-70b-instruct' is in deprecated state from 2024-12-02 until 2025-02-03. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n",
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Querying using vector database...\n",
      "Debug: Raw response from vector DB: {'query': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nNone<|eot_id|><|start_header_id|>user<|end_header_id|>\\nHow to integrate the specified features into the project components?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n', 'result': \" \\n\\nTo integrate the specified features into the project components, you can follow these steps:\\n\\n1. **Import the `greet` function**: In the `app.py` file, you've already imported the `greet` function from `utils.helpers` module. This allows you to use the `greet` function in your `main` function.\\n\\n2. **Use the `greet` function**: In the `main` function, you're using the `greet` function to generate a greeting message based on the user's input.\\n\\n3. **Organize project structure**: The project structure seems to be organized, with the `app.py` file as the main entry point, and the `utils` directory containing helper modules like `helpers.py`.\\n\\n4. **Manage dependencies**: The `requirements.txt` file lists the project dependencies, in this case, `flask`. You can install these dependencies using `pip install -r requirements.txt`.\\n\\nHowever, without more information about the specific features you want to integrate, it's difficult to provide more detailed guidance. If you could provide more context or clarify what features you're trying to integrate, I'd be happy to help further.\"}\n",
      "Debug: Extracted result: To integrate the specified features into the project components, you can follow these steps:\n",
      "\n",
      "1. **Import the `greet` function**: In the `app.py` file, you've already imported the `greet` function from `utils.helpers` module. This allows you to use the `greet` function in your `main` function.\n",
      "\n",
      "2. **Use the `greet` function**: In the `main` function, you're using the `greet` function to generate a greeting message based on the user's input.\n",
      "\n",
      "3. **Organize project structure**: The project structure seems to be organized, with the `app.py` file as the main entry point, and the `utils` directory containing helper modules like `helpers.py`.\n",
      "\n",
      "4. **Manage dependencies**: The `requirements.txt` file lists the project dependencies, in this case, `flask`. You can install these dependencies using `pip install -r requirements.txt`.\n",
      "\n",
      "However, without more information about the specific features you want to integrate, it's difficult to provide more detailed guidance. If you could provide more context or clarify what features you're trying to integrate, I'd be happy to help further.\n",
      "Generated Response:\n",
      "To integrate the specified features into the project components, you can follow these steps:\n",
      "\n",
      "1. **Import the `greet` function**: In the `app.py` file, you've already imported the `greet` function from `utils.helpers` module. This allows you to use the `greet` function in your `main` function.\n",
      "\n",
      "2. **Use the `greet` function**: In the `main` function, you're using the `greet` function to generate a greeting message based on the user's input.\n",
      "\n",
      "3. **Organize project structure**: The project structure seems to be organized, with the `app.py` file as the main entry point, and the `utils` directory containing helper modules like `helpers.py`.\n",
      "\n",
      "4. **Manage dependencies**: The `requirements.txt` file lists the project dependencies, in this case, `flask`. You can install these dependencies using `pip install -r requirements.txt`.\n",
      "\n",
      "However, without more information about the specific features you want to integrate, it's difficult to provide more detailed guidance. If you could provide more context or clarify what features you're trying to integrate, I'd be happy to help further.\n"
     ]
    }
   ],
   "source": [
    "from src.vector_database.db_load import load_vector_db\n",
    "from src.models.llm_inference import query_llm\n",
    "\n",
    "prompt = \"How to integrate the specified features into the project components?\"\n",
    "# Define the persistent directory and collection name\n",
    "persist_directory = \"./notebooks/chroma_db\"\n",
    "collection_name = \"my_vector_collection\"\n",
    "# Load the vector database\n",
    "vector_db = load_vector_db(collection_name, persist_directory)\n",
    "# Query the vector database using the `query_llm` function\n",
    "response = query_llm(user_input=prompt, vector_db=vector_db)\n",
    "# Print the response\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Received prompt: Can you print the tree of the project\n",
      "Debug: Vector DB provided: True\n",
      "Debug: Formatted prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "None<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "Can you print the tree of the project<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Blog/Building-LLM-from-Scratch-in-Python/.venv/lib/python3.10/site-packages/ibm_watsonx_ai/foundation_models/utils/utils.py:415: LifecycleWarning: Model 'meta-llama/llama-3-70b-instruct' is in deprecated state from 2024-12-02 until 2025-02-03. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(\n",
      "Number of requested results 5 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug: Querying using vector database...\n",
      "Debug: Raw response from vector DB: {'query': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nNone<|eot_id|><|start_header_id|>user<|end_header_id|>\\nCan you print the tree of the project<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n', 'result': ' \\n\\nHere is the tree of the project:\\n\\n```\\nproject_old\\napp.py\\nutils\\nhelpers.py\\nrequirements.txt\\n```\\n\\nLet me know if you have any other questions! üòä\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f'}\n",
      "Debug: Extracted result: Here is the tree of the project:\n",
      "\n",
      "```\n",
      "project_old\n",
      "app.py\n",
      "utils\n",
      "helpers.py\n",
      "requirements.txt\n",
      "```\n",
      "\n",
      "Let me know if you have any other questions! üòäÛ†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø\n",
      "Generated Response:\n",
      "Here is the tree of the project:\n",
      "\n",
      "```\n",
      "project_old\n",
      "app.py\n",
      "utils\n",
      "helpers.py\n",
      "requirements.txt\n",
      "```\n",
      "\n",
      "Let me know if you have any other questions! üòäÛ†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you print the tree of the project\"\n",
    "# Define the persistent directory and collection name\n",
    "persist_directory = \"./notebooks/chroma_db\"\n",
    "collection_name = \"my_vector_collection\"\n",
    "# Load the vector database\n",
    "vector_db = load_vector_db(collection_name, persist_directory)\n",
    "# Query the vector database using the `query_llm` function\n",
    "response = query_llm(user_input=prompt, vector_db=vector_db)\n",
    "# Print the response\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
