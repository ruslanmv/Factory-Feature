{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "from src.analysis.project_parser import parse_project\n",
    "from src.analysis.dependency_resolver import resolve_dependencies\n",
    "from src.analysis.feature_mapper import map_features_to_components\n",
    "from src.generation.project_generator import generate_project\n",
    "from src.generation.feature_integration import integrate_features\n",
    "\n",
    "from src.models.prompt_templates import get_prompt_template\n",
    "from src.utils.logger import logger\n",
    "from src.utils.file_operations import read_file, write_file\n",
    "from src.vector_database.db_builder import build_vector_database\n",
    "from src.vector_database.db_query import query_vector_database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.llm_inference import query_llm\n",
    "def main():\n",
    "    # Prompt for the model\n",
    "    prompt = \"Who is man's best friend?\"\n",
    "    # Query the LLM with or without vector database\n",
    "    print(\"Querying the LLM...\")\n",
    "    try:\n",
    "        response = query_llm(prompt)\n",
    "        print(\"Generated Response:\")\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM query: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from chromadb import PersistentClient\n",
    "import torch\n",
    "\n",
    "# Load credentials from .env file\n",
    "load_dotenv()\n",
    "WATSONX_APIKEY = os.getenv(\"WATSONX_APIKEY\")\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "\n",
    "if not WATSONX_APIKEY or not PROJECT_ID:\n",
    "    raise ValueError(\"API key or Project ID is missing. Please check your .env file.\")\n",
    "\n",
    "def get_lang_chain_model(model_type, max_tokens, min_tokens, decoding_method, temperature):\n",
    "    \"\"\"\n",
    "    Initializes and returns a WatsonxLLM instance with the specified parameters.\n",
    "    \"\"\"\n",
    "    return WatsonxLLM(\n",
    "        model_id=model_type,\n",
    "        url=\"https://eu-gb.ml.cloud.ibm.com\",\n",
    "        project_id=PROJECT_ID,\n",
    "        params={\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"min_new_tokens\": min_tokens,\n",
    "            \"decoding_method\": decoding_method,\n",
    "            \"temperature\": temperature,\n",
    "        },\n",
    "    )\n",
    "\n",
    "def answer_questions_from_dataframe(question, collection_name, persist_directory=\"./notebooks/chroma_db\"):\n",
    "    \"\"\"\n",
    "    Answers a question using a LangChain model and retrieves relevant documents from a Chroma collection.\n",
    "    \"\"\"\n",
    "    # Specify model parameters\n",
    "    model_type = \"meta-llama/llama-3-1-70b-instruct\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 100\n",
    "    decoding_method = \"greedy\"\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Initialize the WatsonxLLM model\n",
    "    model = get_lang_chain_model(model_type, max_tokens, min_tokens, decoding_method, temperature)\n",
    "\n",
    "    # Use the same embedding model as in create_embeddings_and_store for consistency\n",
    "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_kwargs = {'device': device}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    hf = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)\n",
    "\n",
    "    # Initialize Chroma Persistent Client\n",
    "    chroma_client = PersistentClient(path=persist_directory)\n",
    "\n",
    "    # Ensure the collection exists\n",
    "    if collection_name not in [col.name for col in chroma_client.list_collections()]:\n",
    "        raise ValueError(f\"Collection '{collection_name}' does not exist. Make sure it is created.\")\n",
    "    collection = chroma_client.get_collection(name=collection_name)\n",
    "\n",
    "    # Use Chroma vectorstore with the given collection\n",
    "    vector_store = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=hf,\n",
    "        client=chroma_client,\n",
    "    )\n",
    "\n",
    "    # Create a retriever from the vectorstore\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "    # Build the RetrievalQA chain\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        # input_key=\"question\"  # This line is not needed\n",
    "    )\n",
    "\n",
    "    # Run the chain with the question\n",
    "    # response_text = chain.invoke({\"question\": question})  # Incorrect key\n",
    "    response_text = chain.invoke({\"query\": question})  # Correct key\n",
    "\n",
    "    print(\"--------------------------------- Generated response -----------------------------------\")\n",
    "    print(response_text)\n",
    "    print(\"*********************************************************************************************\")\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# Example Usage\n",
    "question = \"How to integrate the specified features into the project components?\"\n",
    "\n",
    "# Ensure consistent settings\n",
    "persist_directory = \"./notebooks/chroma_db\"\n",
    "collection_name = \"my_vector_collection\"\n",
    "\n",
    "response = answer_questions_from_dataframe(question, collection_name, persist_directory)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.llm_inference import query_llm\n",
    "# Prompt for the model\n",
    "prompt = \"Who is man's best friend?\"\n",
    "# Query the LLM with or without vector database\n",
    "print(\"Querying the LLM...\")\n",
    "response = query_llm(prompt) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vector_database.db_load import load_vector_db\n",
    "from src.models.llm_inference import query_llm\n",
    "\n",
    "# Define the collection name and persist directory for the vector database\n",
    "COLLECTION_NAME = \"my_vector_collection\"\n",
    "PERSIST_DIRECTORY = \"./notebooks/chroma_db\"\n",
    "\n",
    "# Load the vector database\n",
    "print(\"Loading vector database...\")\n",
    "try:\n",
    "    vector_db = load_vector_db(collection_name=COLLECTION_NAME, persist_directory=PERSIST_DIRECTORY)\n",
    "    print(\"Vector database loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load vector database: {e}\")\n",
    "    vector_db = None\n",
    "\n",
    "# Prompt for the model\n",
    "\n",
    "question = \"How to integrate the specified features into the project components?\"\n",
    "\n",
    "# Query the LLM with or without vector database\n",
    "print(\"Querying the LLM...\")\n",
    "response = query_llm(prompt, vector_db=vector_db) \n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vector_database.db_load import load_vector_db\n",
    "from src.models.llm_inference import query_llm\n",
    "\n",
    "prompt = \"How to integrate the specified features into the project components?\"\n",
    "# Define the persistent directory and collection name\n",
    "persist_directory = \"./notebooks/chroma_db\"\n",
    "collection_name = \"my_vector_collection\"\n",
    "\n",
    "# Load the vector database\n",
    "vector_db = load_vector_db(collection_name, persist_directory)\n",
    "\n",
    "# Query the vector database using the `query_llm` function\n",
    "response = query_llm(prompt=prompt, vector_db=vector_db)\n",
    "\n",
    "# Print the response\n",
    "print(\"Generated Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"Add a user authentication feature to the project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting the Factory Feature program\")\n",
    "# Define paths\n",
    "old_project_path = \"project_old\"\n",
    "new_project_path = \"project_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Parse the old project\n",
    "logger.info(\"Parsing the project structure\")\n",
    "project_data = parse_project(old_project_path)\n",
    "vector_db = build_vector_database(project_data)\n",
    "logger.info(\"Vector database created for the project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Resolve dependencies\n",
    "logger.info(\"Resolving project dependencies\")\n",
    "dependencies = resolve_dependencies(old_project_path)\n",
    "logger.info(f\"Dependencies found: {dependencies.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Map features to project components\n",
    "logger.info(\"Mapping feature request to project components\")\n",
    "feature_template = get_prompt_template()\n",
    "project_context = \"\\n\".join([f\"{key}: {value}\" for key, value in dependencies.items()])\n",
    "feature_request = feature_template.format(feature_request=prompt, project_context=project_context)\n",
    "relevant_docs = query_vector_database(vector_db, feature_request)\n",
    "logger.info(f\"Relevant documents for the feature request: {[doc.metadata['source'] for doc in relevant_docs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate new project structure\n",
    "logger.info(\"Generating the updated project structure\")\n",
    "if os.path.exists(new_project_path):\n",
    "    shutil.rmtree(new_project_path)\n",
    "shutil.copytree(old_project_path, new_project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Integrate new features\n",
    "logger.info(\"Integrating new features into the project\")\n",
    "for doc in relevant_docs:\n",
    "    integrate_features([doc.metadata[\"source\"]], f\"# Feature: {prompt}\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Generate additional files or modify existing ones\n",
    "feature_instructions = [\n",
    "    {\"file\": os.path.relpath(doc.metadata[\"source\"], old_project_path), \"content\": f\"# Feature: {prompt}\\n{doc.page_content}\"}\n",
    "    for doc in relevant_docs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_project(old_project_path, new_project_path, feature_instructions)\n",
    "logger.info(\"Feature integration completed\")\n",
    "print(f\"Project updated with the feature: {prompt}\")\n",
    "print(f\"Updated project saved in: {new_project_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
